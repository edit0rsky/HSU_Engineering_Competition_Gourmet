{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ba5f00d-1792-4ddc-ab20-61af131431f6",
   "metadata": {},
   "source": [
    "dataset_fl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20679bc0-ac71-499a-933c-de7fd190a939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Vocabulary size: 45967\n",
      "\n",
      "--- Starting Hyperparameter Search with 10 trials ---\n",
      "\n",
      "--- Trial 1/10 ---\n",
      "Parameters: {'embedding_dim': 32, 'text_encoder_hidden_dim': 128, 'learning_rate': 0.0005, 'batch_size': 512, 'user_item_mlp_dims': [128, 64], 'final_mlp_dims': [128, 64], 'dropout_rate': 0.2}\n",
      "  Trial 1 Test RMSE: 0.9486\n",
      "  --> New best RMSE found: 0.9486 with params: {'embedding_dim': 32, 'text_encoder_hidden_dim': 128, 'learning_rate': 0.0005, 'batch_size': 512, 'user_item_mlp_dims': [128, 64], 'final_mlp_dims': [128, 64], 'dropout_rate': 0.2}\n",
      "\n",
      "--- Trial 2/10 ---\n",
      "Parameters: {'embedding_dim': 128, 'text_encoder_hidden_dim': 64, 'learning_rate': 0.002, 'batch_size': 128, 'user_item_mlp_dims': [64, 32], 'final_mlp_dims': [64, 32], 'dropout_rate': 0.3}\n",
      "  Trial 2 Test RMSE: 0.9620\n",
      "\n",
      "--- Trial 3/10 ---\n",
      "Parameters: {'embedding_dim': 64, 'text_encoder_hidden_dim': 64, 'learning_rate': 0.001, 'batch_size': 256, 'user_item_mlp_dims': [128, 64], 'final_mlp_dims': [128, 64], 'dropout_rate': 0.3}\n",
      "  Trial 3 Test RMSE: 0.9566\n",
      "\n",
      "--- Trial 4/10 ---\n",
      "Parameters: {'embedding_dim': 32, 'text_encoder_hidden_dim': 256, 'learning_rate': 0.0005, 'batch_size': 512, 'user_item_mlp_dims': [64, 32], 'final_mlp_dims': [128, 64], 'dropout_rate': 0.2}\n",
      "  Trial 4 Test RMSE: 0.9113\n",
      "  --> New best RMSE found: 0.9113 with params: {'embedding_dim': 32, 'text_encoder_hidden_dim': 256, 'learning_rate': 0.0005, 'batch_size': 512, 'user_item_mlp_dims': [64, 32], 'final_mlp_dims': [128, 64], 'dropout_rate': 0.2}\n",
      "\n",
      "--- Trial 5/10 ---\n",
      "Parameters: {'embedding_dim': 128, 'text_encoder_hidden_dim': 256, 'learning_rate': 0.002, 'batch_size': 128, 'user_item_mlp_dims': [128, 64], 'final_mlp_dims': [128, 64], 'dropout_rate': 0.2}\n",
      "  Trial 5 Test RMSE: 0.9315\n",
      "\n",
      "--- Trial 6/10 ---\n",
      "Parameters: {'embedding_dim': 32, 'text_encoder_hidden_dim': 64, 'learning_rate': 0.002, 'batch_size': 512, 'user_item_mlp_dims': [64, 32], 'final_mlp_dims': [64, 32], 'dropout_rate': 0.3}\n",
      "  Trial 6 Test RMSE: 0.9611\n",
      "\n",
      "--- Trial 7/10 ---\n",
      "Parameters: {'embedding_dim': 64, 'text_encoder_hidden_dim': 128, 'learning_rate': 0.0005, 'batch_size': 256, 'user_item_mlp_dims': [256, 128], 'final_mlp_dims': [128, 64], 'dropout_rate': 0.1}\n",
      "  Trial 7 Test RMSE: 0.9310\n",
      "\n",
      "--- Trial 8/10 ---\n",
      "Parameters: {'embedding_dim': 64, 'text_encoder_hidden_dim': 128, 'learning_rate': 0.0005, 'batch_size': 512, 'user_item_mlp_dims': [128, 64], 'final_mlp_dims': [32, 16], 'dropout_rate': 0.2}\n",
      "  Trial 8 Test RMSE: 0.9670\n",
      "\n",
      "--- Trial 9/10 ---\n",
      "Parameters: {'embedding_dim': 32, 'text_encoder_hidden_dim': 128, 'learning_rate': 0.001, 'batch_size': 512, 'user_item_mlp_dims': [256, 128], 'final_mlp_dims': [32, 16], 'dropout_rate': 0.3}\n",
      "  Trial 9 Test RMSE: 0.9281\n",
      "\n",
      "--- Trial 10/10 ---\n",
      "Parameters: {'embedding_dim': 32, 'text_encoder_hidden_dim': 128, 'learning_rate': 0.001, 'batch_size': 512, 'user_item_mlp_dims': [128, 64], 'final_mlp_dims': [128, 64], 'dropout_rate': 0.3}\n",
      "  Trial 10 Test RMSE: 0.9498\n",
      "\n",
      "--- Hyperparameter Search Completed ---\n",
      "Best Parameters found: {'embedding_dim': 32, 'text_encoder_hidden_dim': 256, 'learning_rate': 0.0005, 'batch_size': 512, 'user_item_mlp_dims': [64, 32], 'final_mlp_dims': [128, 64], 'dropout_rate': 0.2}\n",
      "\n",
      "--- Starting 5 runs with Best Parameters ---\n",
      "Best Parameters: {'embedding_dim': 32, 'text_encoder_hidden_dim': 256, 'learning_rate': 0.0005, 'batch_size': 512, 'user_item_mlp_dims': [64, 32], 'final_mlp_dims': [128, 64], 'dropout_rate': 0.2}\n",
      "\n",
      "--- Run 1/5 (Random State: 42) ---\n",
      "Run 1 Performance on Test Set:\n",
      "  RMSE: 0.9102\n",
      "  MAE: 0.6747\n",
      "\n",
      "--- Run 2/5 (Random State: 43) ---\n",
      "Run 2 Performance on Test Set:\n",
      "  RMSE: 0.8990\n",
      "  MAE: 0.6467\n",
      "\n",
      "--- Run 3/5 (Random State: 44) ---\n",
      "Run 3 Performance on Test Set:\n",
      "  RMSE: 0.9041\n",
      "  MAE: 0.6695\n",
      "\n",
      "--- Run 4/5 (Random State: 45) ---\n",
      "Run 4 Performance on Test Set:\n",
      "  RMSE: 0.9127\n",
      "  MAE: 0.6575\n",
      "\n",
      "--- Run 5/5 (Random State: 46) ---\n",
      "Run 5 Performance on Test Set:\n",
      "  RMSE: 0.9127\n",
      "  MAE: 0.6579\n",
      "\n",
      "--- Average Performance over 5 Runs ---\n",
      "Average RMSE: 0.9077 +/- 0.0053\n",
      "Average MAE: 0.6613 +/- 0.0098\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import re\n",
    "from collections import Counter\n",
    "import random\n",
    "import json\n",
    "\n",
    "# --- 장치 설정 (GPU 사용 가능 시) ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --- 데이터 전처리 및 어휘 구축 ---\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
    "    return text.split()\n",
    "\n",
    "class Vocabulary:\n",
    "    def __init__(self, min_freq):\n",
    "        self.stoi = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
    "        self.itos = {0: \"<PAD>\", 1: \"<UNK>\"}\n",
    "        self.freq = Counter()\n",
    "        self.min_freq = min_freq\n",
    "    \n",
    "    def build_vocabulary(self, text_list):\n",
    "        for text in text_list:\n",
    "            self.freq.update(text)\n",
    "        \n",
    "        idx = 2\n",
    "        for word, count in self.freq.items():\n",
    "            if count >= self.min_freq:\n",
    "                self.stoi[word] = idx\n",
    "                self.itos[idx] = word\n",
    "                idx += 1\n",
    "    \n",
    "    def numericalize(self, text):\n",
    "        return [self.stoi.get(token, self.stoi[\"<UNK>\"]) for token in text]\n",
    "\n",
    "# --- PyTorch Dataset 및 DataLoader 정의 ---\n",
    "class AFRAMDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.user_ids = torch.tensor(df['user_encoded'].values, dtype=torch.long)\n",
    "        self.business_ids = torch.tensor(df['business_encoded'].values, dtype=torch.long)\n",
    "        self.reviews = torch.tensor(np.array(df['numericalized_text'].tolist()), dtype=torch.long)\n",
    "        self.stars = torch.tensor(df['review_stars'].values, dtype=torch.float)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.stars)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.user_ids[idx], self.business_ids[idx], self.reviews[idx], self.stars[idx]\n",
    "\n",
    "# --- AFRAM 모델 아키텍처 정의 ---\n",
    "class TextEncoderWithAttention(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, dropout_rate):\n",
    "        super(TextEncoderWithAttention, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.conv = nn.Conv1d(in_channels=embedding_dim, out_channels=hidden_dim, kernel_size=3, padding=1)\n",
    "        self.lstm = nn.LSTM(hidden_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.attn_proj = nn.Linear(hidden_dim * 2, hidden_dim * 2)\n",
    "        self.v = nn.Parameter(torch.rand(hidden_dim * 2, 1))\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, text_seq):\n",
    "        embedded = self.embedding(text_seq)\n",
    "        embedded = embedded.permute(0, 2, 1)\n",
    "        conv_out = torch.relu(self.conv(embedded))\n",
    "        conv_out = conv_out.permute(0, 2, 1)\n",
    "        lstm_out, _ = self.lstm(self.dropout(conv_out))\n",
    "        attn_weights = torch.tanh(self.attn_proj(lstm_out))\n",
    "        v_expanded = self.v.unsqueeze(0).expand(attn_weights.shape[0], -1, -1)\n",
    "        scores = torch.bmm(attn_weights, v_expanded)\n",
    "        attention_weights = torch.softmax(scores, dim=1)\n",
    "        context_vector = torch.sum(lstm_out * attention_weights, dim=1)\n",
    "        return context_vector\n",
    "\n",
    "class AFRAMModel(nn.Module):\n",
    "    def __init__(self, num_users, num_businesses, vocab_size, embedding_dim,\n",
    "                 text_encoder_hidden_dim, user_item_mlp_dims, final_mlp_dims, dropout_rate):\n",
    "        super(AFRAMModel, self).__init__()\n",
    "        self.user_embedding = nn.Embedding(num_users, embedding_dim)\n",
    "        self.business_embedding = nn.Embedding(num_businesses, embedding_dim)\n",
    "        self.review_encoder = TextEncoderWithAttention(vocab_size, embedding_dim, text_encoder_hidden_dim, dropout_rate)\n",
    "        user_item_mlp_input_dim = embedding_dim * 2\n",
    "        user_item_layers = []\n",
    "        for dim in user_item_mlp_dims:\n",
    "            user_item_layers.append(nn.Linear(user_item_mlp_input_dim, dim))\n",
    "            user_item_layers.append(nn.ReLU())\n",
    "            user_item_mlp_input_dim = dim\n",
    "        self.user_item_mlp = nn.Sequential(*user_item_layers)\n",
    "        self.user_item_mlp_output_dim = user_item_mlp_dims[-1] if user_item_mlp_dims else embedding_dim * 2\n",
    "        final_mlp_input_dim = self.user_item_mlp_output_dim + text_encoder_hidden_dim * 2\n",
    "        final_layers = []\n",
    "        for dim in final_mlp_dims:\n",
    "            final_layers.append(nn.Linear(final_mlp_input_dim, dim))\n",
    "            final_layers.append(nn.ReLU())\n",
    "            final_mlp_input_dim = dim\n",
    "        final_layers.append(nn.Linear(final_mlp_input_dim, 1))\n",
    "        self.prediction_mlp = nn.Sequential(*final_layers)\n",
    "\n",
    "    def forward(self, user_ids, business_ids, reviews):\n",
    "        user_vec = self.user_embedding(user_ids)\n",
    "        business_vec = self.business_embedding(business_ids)\n",
    "        user_item_combined = torch.cat((user_vec, business_vec), dim=1)\n",
    "        user_item_features = self.user_item_mlp(user_item_combined)\n",
    "        review_features = self.review_encoder(reviews)\n",
    "        combined_features = torch.cat((user_item_features, review_features), dim=1)\n",
    "        predicted_rating = self.prediction_mlp(combined_features)\n",
    "        return predicted_rating.reshape(-1) # <-- 이 부분을 수정했습니다.\n",
    "\n",
    "def train_and_evaluate(params, train_df, val_df, test_df, num_users, num_businesses, vocab_size, run_num):\n",
    "    train_loader = DataLoader(AFRAMDataset(train_df), batch_size=params['batch_size'], shuffle=True)\n",
    "    val_loader = DataLoader(AFRAMDataset(val_df), batch_size=params['batch_size'], shuffle=False)\n",
    "    test_loader = DataLoader(AFRAMDataset(test_df), batch_size=params['batch_size'], shuffle=False)\n",
    "    model = AFRAMModel(num_users, num_businesses, vocab_size, params['embedding_dim'],\n",
    "                       params['text_encoder_hidden_dim'], params['user_item_mlp_dims'],\n",
    "                       params['final_mlp_dims'], params['dropout_rate']).to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=params['learning_rate'])\n",
    "    epochs = 50\n",
    "    patience = 7\n",
    "    min_delta = 0.0005\n",
    "    best_val_rmse = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    model_save_path = f'temp_best_model_run_{run_num}.pt'\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for user_ids, business_ids, reviews, stars in train_loader:\n",
    "            user_ids, business_ids, reviews, stars = user_ids.to(device), business_ids.to(device), reviews.to(device), stars.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(user_ids, business_ids, reviews)\n",
    "            loss = criterion(predictions, stars)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        model.eval()\n",
    "        val_predictions = []\n",
    "        val_true_ratings = []\n",
    "        with torch.no_grad():\n",
    "            for user_ids, business_ids, reviews, stars in val_loader:\n",
    "                user_ids, business_ids, reviews, stars = user_ids.to(device), business_ids.to(device), reviews.to(device), stars.to(device)\n",
    "                predictions = model(user_ids, business_ids, reviews)\n",
    "                val_predictions.extend(predictions.tolist())\n",
    "                val_true_ratings.extend(stars.tolist())\n",
    "        \n",
    "        current_val_rmse = np.sqrt(mean_squared_error(val_true_ratings, val_predictions))\n",
    "        \n",
    "        if current_val_rmse < best_val_rmse - min_delta:\n",
    "            best_val_rmse = current_val_rmse\n",
    "            epochs_no_improve = 0\n",
    "            torch.save(model.state_dict(), model_save_path)\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve == patience:\n",
    "                break\n",
    "    \n",
    "    if os.path.exists(model_save_path):\n",
    "        model.load_state_dict(torch.load(model_save_path))\n",
    "    \n",
    "    model.eval()\n",
    "    test_predictions = []\n",
    "    true_ratings = []\n",
    "    with torch.no_grad():\n",
    "        for user_ids, business_ids, reviews, stars in test_loader:\n",
    "            user_ids, business_ids, reviews, stars = user_ids.to(device), business_ids.to(device), reviews.to(device), stars.to(device)\n",
    "            predictions = model(user_ids, business_ids, reviews)\n",
    "            test_predictions.extend(predictions.tolist())\n",
    "            true_ratings.extend(stars.tolist())\n",
    "\n",
    "    mse = mean_squared_error(true_ratings, test_predictions)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(true_ratings, test_predictions)\n",
    "\n",
    "    if os.path.exists(model_save_path):\n",
    "        os.remove(model_save_path)\n",
    "\n",
    "    return mse, rmse, mae\n",
    "\n",
    "def main():\n",
    "    # --- 1. 데이터 로드 및 전처리 ---\n",
    "    try:\n",
    "        df = pd.read_json('dataset_fl.json', lines=True)\n",
    "    except ValueError:\n",
    "        df = pd.read_json('dataset_fl.json')\n",
    "    df_processed = df[['user_id', 'business_id', 'review_stars', 'text']].copy()\n",
    "    user_encoder = LabelEncoder()\n",
    "    business_encoder = LabelEncoder()\n",
    "    df_processed.loc[:, 'user_encoded'] = user_encoder.fit_transform(df_processed['user_id'])\n",
    "    df_processed.loc[:, 'business_encoded'] = business_encoder.fit_transform(df_processed['business_id'])\n",
    "    num_users = len(user_encoder.classes_)\n",
    "    num_businesses = len(business_encoder.classes_)\n",
    "    all_texts = df_processed['text'].apply(preprocess_text).tolist()\n",
    "    min_word_freq = 5\n",
    "    vocab = Vocabulary(min_word_freq)\n",
    "    vocab.build_vocabulary(all_texts)\n",
    "    vocab_size = len(vocab.stoi)\n",
    "    print(f\"Vocabulary size: {vocab_size}\")\n",
    "    MAX_REVIEW_LEN = 100\n",
    "    df_processed.loc[:, 'numericalized_text'] = df_processed['text'].apply(vocab.numericalize)\n",
    "    df_processed['numericalized_text'] = df_processed['numericalized_text'].apply(\n",
    "        lambda x: x[:MAX_REVIEW_LEN] if len(x) > MAX_REVIEW_LEN else x + [vocab.stoi[\"<PAD>\"]] * (MAX_REVIEW_LEN - len(x))\n",
    "    )\n",
    "\n",
    "    # --- 2. 하이퍼파라미터 탐색 (Random Search) ---\n",
    "    param_grid = {\n",
    "        'embedding_dim': [32, 64, 128],\n",
    "        'text_encoder_hidden_dim': [64, 128, 256],\n",
    "        'learning_rate': [0.0005, 0.001, 0.002],\n",
    "        'batch_size': [128, 256, 512],\n",
    "        'user_item_mlp_dims': [[64, 32], [128, 64], [256, 128]],\n",
    "        'final_mlp_dims': [[32, 16], [64, 32], [128, 64]],\n",
    "        'dropout_rate': [0.1, 0.2, 0.3]\n",
    "    }\n",
    "    num_trials = 10\n",
    "    best_params = None\n",
    "    best_val_rmse = float('inf')\n",
    "    \n",
    "    print(f\"\\n--- Starting Hyperparameter Search with {num_trials} trials ---\")\n",
    "    train_val_df, test_df_hp = train_test_split(df_processed, test_size=0.2, random_state=42)\n",
    "    train_df_hp, val_df_hp = train_test_split(train_val_df, test_size=1/8, random_state=42)\n",
    "\n",
    "    for trial_num in range(num_trials):\n",
    "        current_params = {k: random.choice(v) for k, v in param_grid.items()}\n",
    "        print(f\"\\n--- Trial {trial_num + 1}/{num_trials} ---\")\n",
    "        print(f\"Parameters: {current_params}\")\n",
    "        \n",
    "        mse, rmse, mae = train_and_evaluate(current_params, train_df_hp, val_df_hp, test_df_hp,\n",
    "                                            num_users, num_businesses, vocab_size, trial_num)\n",
    "        \n",
    "        print(f\"  Trial {trial_num+1} Test RMSE: {rmse:.4f}\")\n",
    "        \n",
    "        if rmse < best_val_rmse:\n",
    "            best_val_rmse = rmse\n",
    "            best_params = current_params\n",
    "            print(f\"  --> New best RMSE found: {best_val_rmse:.4f} with params: {best_params}\")\n",
    "\n",
    "    print(f\"\\n--- Hyperparameter Search Completed ---\")\n",
    "    print(f\"Best Parameters found: {best_params}\")\n",
    "\n",
    "    # --- 3. 최적 파라미터로 모델 5회 반복 학습 및 평균 성능 계산 ---\n",
    "    if best_params:\n",
    "        all_rmse = []\n",
    "        all_mae = []\n",
    "        num_runs = 5\n",
    "        \n",
    "        print(f\"\\n--- Starting {num_runs} runs with Best Parameters ---\")\n",
    "        print(f\"Best Parameters: {best_params}\")\n",
    "        \n",
    "        for i in range(num_runs):\n",
    "            current_random_state = 42 + i\n",
    "            print(f\"\\n--- Run {i+1}/{num_runs} (Random State: {current_random_state}) ---\")\n",
    "            train_val_df, test_df = train_test_split(df_processed, test_size=0.2, random_state=current_random_state)\n",
    "            train_df, val_df = train_test_split(train_val_df, test_size=1/8, random_state=current_random_state)\n",
    "            mse, rmse, mae = train_and_evaluate(best_params, train_df, val_df, test_df,\n",
    "                                                num_users, num_businesses, vocab_size, i)\n",
    "            \n",
    "            print(f\"Run {i+1} Performance on Test Set:\")\n",
    "            print(f\"  RMSE: {rmse:.4f}\")\n",
    "            print(f\"  MAE: {mae:.4f}\")\n",
    "\n",
    "            all_rmse.append(rmse)\n",
    "            all_mae.append(mae)\n",
    "\n",
    "        print(f\"\\n--- Average Performance over {num_runs} Runs ---\")\n",
    "        print(f\"Average RMSE: {np.mean(all_rmse):.4f} +/- {np.std(all_rmse):.4f}\")\n",
    "        print(f\"Average MAE: {np.mean(all_mae):.4f} +/- {np.std(all_mae):.4f}\")\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65d7f0b-514c-4c6d-b333-f3bed18b760c",
   "metadata": {},
   "source": [
    "dataset_la"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e0d23cf-c61d-4b41-b8b4-4b82eece6f52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Vocabulary size: 37302\n",
      "\n",
      "--- Starting Hyperparameter Search with 10 trials ---\n",
      "\n",
      "--- Trial 1/10 ---\n",
      "Parameters: {'embedding_dim': 64, 'text_encoder_hidden_dim': 64, 'learning_rate': 0.001, 'batch_size': 256, 'user_item_mlp_dims': [256, 128], 'final_mlp_dims': [32, 16], 'dropout_rate': 0.3}\n",
      "  Trial 1 Test RMSE: 0.9842\n",
      "  --> New best RMSE found: 0.9842 with params: {'embedding_dim': 64, 'text_encoder_hidden_dim': 64, 'learning_rate': 0.001, 'batch_size': 256, 'user_item_mlp_dims': [256, 128], 'final_mlp_dims': [32, 16], 'dropout_rate': 0.3}\n",
      "\n",
      "--- Trial 2/10 ---\n",
      "Parameters: {'embedding_dim': 64, 'text_encoder_hidden_dim': 128, 'learning_rate': 0.001, 'batch_size': 512, 'user_item_mlp_dims': [128, 64], 'final_mlp_dims': [32, 16], 'dropout_rate': 0.2}\n",
      "  Trial 2 Test RMSE: 0.9947\n",
      "\n",
      "--- Trial 3/10 ---\n",
      "Parameters: {'embedding_dim': 32, 'text_encoder_hidden_dim': 64, 'learning_rate': 0.001, 'batch_size': 128, 'user_item_mlp_dims': [256, 128], 'final_mlp_dims': [32, 16], 'dropout_rate': 0.1}\n",
      "  Trial 3 Test RMSE: 0.9249\n",
      "  --> New best RMSE found: 0.9249 with params: {'embedding_dim': 32, 'text_encoder_hidden_dim': 64, 'learning_rate': 0.001, 'batch_size': 128, 'user_item_mlp_dims': [256, 128], 'final_mlp_dims': [32, 16], 'dropout_rate': 0.1}\n",
      "\n",
      "--- Trial 4/10 ---\n",
      "Parameters: {'embedding_dim': 128, 'text_encoder_hidden_dim': 256, 'learning_rate': 0.002, 'batch_size': 256, 'user_item_mlp_dims': [64, 32], 'final_mlp_dims': [128, 64], 'dropout_rate': 0.2}\n",
      "  Trial 4 Test RMSE: 0.9587\n",
      "\n",
      "--- Trial 5/10 ---\n",
      "Parameters: {'embedding_dim': 128, 'text_encoder_hidden_dim': 128, 'learning_rate': 0.002, 'batch_size': 512, 'user_item_mlp_dims': [128, 64], 'final_mlp_dims': [32, 16], 'dropout_rate': 0.2}\n",
      "  Trial 5 Test RMSE: 0.9749\n",
      "\n",
      "--- Trial 6/10 ---\n",
      "Parameters: {'embedding_dim': 128, 'text_encoder_hidden_dim': 256, 'learning_rate': 0.0005, 'batch_size': 128, 'user_item_mlp_dims': [64, 32], 'final_mlp_dims': [128, 64], 'dropout_rate': 0.2}\n",
      "  Trial 6 Test RMSE: 0.9346\n",
      "\n",
      "--- Trial 7/10 ---\n",
      "Parameters: {'embedding_dim': 32, 'text_encoder_hidden_dim': 128, 'learning_rate': 0.0005, 'batch_size': 128, 'user_item_mlp_dims': [256, 128], 'final_mlp_dims': [64, 32], 'dropout_rate': 0.2}\n",
      "  Trial 7 Test RMSE: 0.9282\n",
      "\n",
      "--- Trial 8/10 ---\n",
      "Parameters: {'embedding_dim': 128, 'text_encoder_hidden_dim': 256, 'learning_rate': 0.0005, 'batch_size': 512, 'user_item_mlp_dims': [128, 64], 'final_mlp_dims': [128, 64], 'dropout_rate': 0.3}\n",
      "  Trial 8 Test RMSE: 0.9727\n",
      "\n",
      "--- Trial 9/10 ---\n",
      "Parameters: {'embedding_dim': 32, 'text_encoder_hidden_dim': 256, 'learning_rate': 0.0005, 'batch_size': 512, 'user_item_mlp_dims': [64, 32], 'final_mlp_dims': [64, 32], 'dropout_rate': 0.1}\n",
      "  Trial 9 Test RMSE: 0.9317\n",
      "\n",
      "--- Trial 10/10 ---\n",
      "Parameters: {'embedding_dim': 64, 'text_encoder_hidden_dim': 64, 'learning_rate': 0.002, 'batch_size': 128, 'user_item_mlp_dims': [64, 32], 'final_mlp_dims': [32, 16], 'dropout_rate': 0.2}\n",
      "  Trial 10 Test RMSE: 0.9627\n",
      "\n",
      "--- Hyperparameter Search Completed ---\n",
      "Best Parameters found: {'embedding_dim': 32, 'text_encoder_hidden_dim': 64, 'learning_rate': 0.001, 'batch_size': 128, 'user_item_mlp_dims': [256, 128], 'final_mlp_dims': [32, 16], 'dropout_rate': 0.1}\n",
      "\n",
      "--- Starting 5 runs with Best Parameters ---\n",
      "Best Parameters: {'embedding_dim': 32, 'text_encoder_hidden_dim': 64, 'learning_rate': 0.001, 'batch_size': 128, 'user_item_mlp_dims': [256, 128], 'final_mlp_dims': [32, 16], 'dropout_rate': 0.1}\n",
      "\n",
      "--- Run 1/5 (Random State: 42) ---\n",
      "Run 1 Performance on Test Set:\n",
      "  RMSE: 0.9470\n",
      "  MAE: 0.7298\n",
      "\n",
      "--- Run 2/5 (Random State: 43) ---\n",
      "Run 2 Performance on Test Set:\n",
      "  RMSE: 0.9308\n",
      "  MAE: 0.7126\n",
      "\n",
      "--- Run 3/5 (Random State: 44) ---\n",
      "Run 3 Performance on Test Set:\n",
      "  RMSE: 0.9277\n",
      "  MAE: 0.7123\n",
      "\n",
      "--- Run 4/5 (Random State: 45) ---\n",
      "Run 4 Performance on Test Set:\n",
      "  RMSE: 0.9329\n",
      "  MAE: 0.7082\n",
      "\n",
      "--- Run 5/5 (Random State: 46) ---\n",
      "Run 5 Performance on Test Set:\n",
      "  RMSE: 0.9436\n",
      "  MAE: 0.7426\n",
      "\n",
      "--- Average Performance over 5 Runs ---\n",
      "Average RMSE: 0.9364 +/- 0.0075\n",
      "Average MAE: 0.7211 +/- 0.0131\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import re\n",
    "from collections import Counter\n",
    "import random\n",
    "import json\n",
    "\n",
    "# --- 장치 설정 (GPU 사용 가능 시) ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --- 데이터 전처리 및 어휘 구축 ---\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
    "    return text.split()\n",
    "\n",
    "class Vocabulary:\n",
    "    def __init__(self, min_freq):\n",
    "        self.stoi = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
    "        self.itos = {0: \"<PAD>\", 1: \"<UNK>\"}\n",
    "        self.freq = Counter()\n",
    "        self.min_freq = min_freq\n",
    "    \n",
    "    def build_vocabulary(self, text_list):\n",
    "        for text in text_list:\n",
    "            self.freq.update(text)\n",
    "        \n",
    "        idx = 2\n",
    "        for word, count in self.freq.items():\n",
    "            if count >= self.min_freq:\n",
    "                self.stoi[word] = idx\n",
    "                self.itos[idx] = word\n",
    "                idx += 1\n",
    "    \n",
    "    def numericalize(self, text):\n",
    "        return [self.stoi.get(token, self.stoi[\"<UNK>\"]) for token in text]\n",
    "\n",
    "# --- PyTorch Dataset 및 DataLoader 정의 ---\n",
    "class AFRAMDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.user_ids = torch.tensor(df['user_encoded'].values, dtype=torch.long)\n",
    "        self.business_ids = torch.tensor(df['business_encoded'].values, dtype=torch.long)\n",
    "        self.reviews = torch.tensor(np.array(df['numericalized_text'].tolist()), dtype=torch.long)\n",
    "        self.stars = torch.tensor(df['review_stars'].values, dtype=torch.float)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.stars)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.user_ids[idx], self.business_ids[idx], self.reviews[idx], self.stars[idx]\n",
    "\n",
    "# --- AFRAM 모델 아키텍처 정의 ---\n",
    "class TextEncoderWithAttention(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, dropout_rate):\n",
    "        super(TextEncoderWithAttention, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.conv = nn.Conv1d(in_channels=embedding_dim, out_channels=hidden_dim, kernel_size=3, padding=1)\n",
    "        self.lstm = nn.LSTM(hidden_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.attn_proj = nn.Linear(hidden_dim * 2, hidden_dim * 2)\n",
    "        self.v = nn.Parameter(torch.rand(hidden_dim * 2, 1))\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, text_seq):\n",
    "        embedded = self.embedding(text_seq)\n",
    "        embedded = embedded.permute(0, 2, 1)\n",
    "        conv_out = torch.relu(self.conv(embedded))\n",
    "        conv_out = conv_out.permute(0, 2, 1)\n",
    "        lstm_out, _ = self.lstm(self.dropout(conv_out))\n",
    "        attn_weights = torch.tanh(self.attn_proj(lstm_out))\n",
    "        v_expanded = self.v.unsqueeze(0).expand(attn_weights.shape[0], -1, -1)\n",
    "        scores = torch.bmm(attn_weights, v_expanded)\n",
    "        attention_weights = torch.softmax(scores, dim=1)\n",
    "        context_vector = torch.sum(lstm_out * attention_weights, dim=1)\n",
    "        return context_vector\n",
    "\n",
    "class AFRAMModel(nn.Module):\n",
    "    def __init__(self, num_users, num_businesses, vocab_size, embedding_dim,\n",
    "                 text_encoder_hidden_dim, user_item_mlp_dims, final_mlp_dims, dropout_rate):\n",
    "        super(AFRAMModel, self).__init__()\n",
    "        self.user_embedding = nn.Embedding(num_users, embedding_dim)\n",
    "        self.business_embedding = nn.Embedding(num_businesses, embedding_dim)\n",
    "        self.review_encoder = TextEncoderWithAttention(vocab_size, embedding_dim, text_encoder_hidden_dim, dropout_rate)\n",
    "        user_item_mlp_input_dim = embedding_dim * 2\n",
    "        user_item_layers = []\n",
    "        for dim in user_item_mlp_dims:\n",
    "            user_item_layers.append(nn.Linear(user_item_mlp_input_dim, dim))\n",
    "            user_item_layers.append(nn.ReLU())\n",
    "            user_item_mlp_input_dim = dim\n",
    "        self.user_item_mlp = nn.Sequential(*user_item_layers)\n",
    "        self.user_item_mlp_output_dim = user_item_mlp_dims[-1] if user_item_mlp_dims else embedding_dim * 2\n",
    "        final_mlp_input_dim = self.user_item_mlp_output_dim + text_encoder_hidden_dim * 2\n",
    "        final_layers = []\n",
    "        for dim in final_mlp_dims:\n",
    "            final_layers.append(nn.Linear(final_mlp_input_dim, dim))\n",
    "            final_layers.append(nn.ReLU())\n",
    "            final_mlp_input_dim = dim\n",
    "        final_layers.append(nn.Linear(final_mlp_input_dim, 1))\n",
    "        self.prediction_mlp = nn.Sequential(*final_layers)\n",
    "\n",
    "    def forward(self, user_ids, business_ids, reviews):\n",
    "        user_vec = self.user_embedding(user_ids)\n",
    "        business_vec = self.business_embedding(business_ids)\n",
    "        user_item_combined = torch.cat((user_vec, business_vec), dim=1)\n",
    "        user_item_features = self.user_item_mlp(user_item_combined)\n",
    "        review_features = self.review_encoder(reviews)\n",
    "        combined_features = torch.cat((user_item_features, review_features), dim=1)\n",
    "        predicted_rating = self.prediction_mlp(combined_features)\n",
    "        return predicted_rating.reshape(-1) # <-- 이 부분을 수정했습니다.\n",
    "\n",
    "def train_and_evaluate(params, train_df, val_df, test_df, num_users, num_businesses, vocab_size, run_num):\n",
    "    train_loader = DataLoader(AFRAMDataset(train_df), batch_size=params['batch_size'], shuffle=True)\n",
    "    val_loader = DataLoader(AFRAMDataset(val_df), batch_size=params['batch_size'], shuffle=False)\n",
    "    test_loader = DataLoader(AFRAMDataset(test_df), batch_size=params['batch_size'], shuffle=False)\n",
    "    model = AFRAMModel(num_users, num_businesses, vocab_size, params['embedding_dim'],\n",
    "                       params['text_encoder_hidden_dim'], params['user_item_mlp_dims'],\n",
    "                       params['final_mlp_dims'], params['dropout_rate']).to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=params['learning_rate'])\n",
    "    epochs = 50\n",
    "    patience = 7\n",
    "    min_delta = 0.0005\n",
    "    best_val_rmse = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    model_save_path = f'temp_best_model_run_{run_num}.pt'\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for user_ids, business_ids, reviews, stars in train_loader:\n",
    "            user_ids, business_ids, reviews, stars = user_ids.to(device), business_ids.to(device), reviews.to(device), stars.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(user_ids, business_ids, reviews)\n",
    "            loss = criterion(predictions, stars)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        model.eval()\n",
    "        val_predictions = []\n",
    "        val_true_ratings = []\n",
    "        with torch.no_grad():\n",
    "            for user_ids, business_ids, reviews, stars in val_loader:\n",
    "                user_ids, business_ids, reviews, stars = user_ids.to(device), business_ids.to(device), reviews.to(device), stars.to(device)\n",
    "                predictions = model(user_ids, business_ids, reviews)\n",
    "                val_predictions.extend(predictions.tolist())\n",
    "                val_true_ratings.extend(stars.tolist())\n",
    "        \n",
    "        current_val_rmse = np.sqrt(mean_squared_error(val_true_ratings, val_predictions))\n",
    "        \n",
    "        if current_val_rmse < best_val_rmse - min_delta:\n",
    "            best_val_rmse = current_val_rmse\n",
    "            epochs_no_improve = 0\n",
    "            torch.save(model.state_dict(), model_save_path)\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve == patience:\n",
    "                break\n",
    "    \n",
    "    if os.path.exists(model_save_path):\n",
    "        model.load_state_dict(torch.load(model_save_path))\n",
    "    \n",
    "    model.eval()\n",
    "    test_predictions = []\n",
    "    true_ratings = []\n",
    "    with torch.no_grad():\n",
    "        for user_ids, business_ids, reviews, stars in test_loader:\n",
    "            user_ids, business_ids, reviews, stars = user_ids.to(device), business_ids.to(device), reviews.to(device), stars.to(device)\n",
    "            predictions = model(user_ids, business_ids, reviews)\n",
    "            test_predictions.extend(predictions.tolist())\n",
    "            true_ratings.extend(stars.tolist())\n",
    "\n",
    "    mse = mean_squared_error(true_ratings, test_predictions)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(true_ratings, test_predictions)\n",
    "\n",
    "    if os.path.exists(model_save_path):\n",
    "        os.remove(model_save_path)\n",
    "\n",
    "    return mse, rmse, mae\n",
    "\n",
    "def main():\n",
    "    # --- 1. 데이터 로드 및 전처리 ---\n",
    "    try:\n",
    "        df = pd.read_json('dataset_la.json', lines=True)\n",
    "    except ValueError:\n",
    "        df = pd.read_json('dataset_la.json')\n",
    "    df_processed = df[['user_id', 'business_id', 'review_stars', 'text']].copy()\n",
    "    user_encoder = LabelEncoder()\n",
    "    business_encoder = LabelEncoder()\n",
    "    df_processed.loc[:, 'user_encoded'] = user_encoder.fit_transform(df_processed['user_id'])\n",
    "    df_processed.loc[:, 'business_encoded'] = business_encoder.fit_transform(df_processed['business_id'])\n",
    "    num_users = len(user_encoder.classes_)\n",
    "    num_businesses = len(business_encoder.classes_)\n",
    "    all_texts = df_processed['text'].apply(preprocess_text).tolist()\n",
    "    min_word_freq = 5\n",
    "    vocab = Vocabulary(min_word_freq)\n",
    "    vocab.build_vocabulary(all_texts)\n",
    "    vocab_size = len(vocab.stoi)\n",
    "    print(f\"Vocabulary size: {vocab_size}\")\n",
    "    MAX_REVIEW_LEN = 100\n",
    "    df_processed.loc[:, 'numericalized_text'] = df_processed['text'].apply(vocab.numericalize)\n",
    "    df_processed['numericalized_text'] = df_processed['numericalized_text'].apply(\n",
    "        lambda x: x[:MAX_REVIEW_LEN] if len(x) > MAX_REVIEW_LEN else x + [vocab.stoi[\"<PAD>\"]] * (MAX_REVIEW_LEN - len(x))\n",
    "    )\n",
    "\n",
    "    # --- 2. 하이퍼파라미터 탐색 (Random Search) ---\n",
    "    param_grid = {\n",
    "        'embedding_dim': [32, 64, 128],\n",
    "        'text_encoder_hidden_dim': [64, 128, 256],\n",
    "        'learning_rate': [0.0005, 0.001, 0.002],\n",
    "        'batch_size': [128, 256, 512],\n",
    "        'user_item_mlp_dims': [[64, 32], [128, 64], [256, 128]],\n",
    "        'final_mlp_dims': [[32, 16], [64, 32], [128, 64]],\n",
    "        'dropout_rate': [0.1, 0.2, 0.3]\n",
    "    }\n",
    "    num_trials = 10\n",
    "    best_params = None\n",
    "    best_val_rmse = float('inf')\n",
    "    \n",
    "    print(f\"\\n--- Starting Hyperparameter Search with {num_trials} trials ---\")\n",
    "    train_val_df, test_df_hp = train_test_split(df_processed, test_size=0.2, random_state=42)\n",
    "    train_df_hp, val_df_hp = train_test_split(train_val_df, test_size=1/8, random_state=42)\n",
    "\n",
    "    for trial_num in range(num_trials):\n",
    "        current_params = {k: random.choice(v) for k, v in param_grid.items()}\n",
    "        print(f\"\\n--- Trial {trial_num + 1}/{num_trials} ---\")\n",
    "        print(f\"Parameters: {current_params}\")\n",
    "        \n",
    "        mse, rmse, mae = train_and_evaluate(current_params, train_df_hp, val_df_hp, test_df_hp,\n",
    "                                            num_users, num_businesses, vocab_size, trial_num)\n",
    "        \n",
    "        print(f\"  Trial {trial_num+1} Test RMSE: {rmse:.4f}\")\n",
    "        \n",
    "        if rmse < best_val_rmse:\n",
    "            best_val_rmse = rmse\n",
    "            best_params = current_params\n",
    "            print(f\"  --> New best RMSE found: {best_val_rmse:.4f} with params: {best_params}\")\n",
    "\n",
    "    print(f\"\\n--- Hyperparameter Search Completed ---\")\n",
    "    print(f\"Best Parameters found: {best_params}\")\n",
    "\n",
    "    # --- 3. 최적 파라미터로 모델 5회 반복 학습 및 평균 성능 계산 ---\n",
    "    if best_params:\n",
    "        all_rmse = []\n",
    "        all_mae = []\n",
    "        num_runs = 5\n",
    "        \n",
    "        print(f\"\\n--- Starting {num_runs} runs with Best Parameters ---\")\n",
    "        print(f\"Best Parameters: {best_params}\")\n",
    "        \n",
    "        for i in range(num_runs):\n",
    "            current_random_state = 42 + i\n",
    "            print(f\"\\n--- Run {i+1}/{num_runs} (Random State: {current_random_state}) ---\")\n",
    "            train_val_df, test_df = train_test_split(df_processed, test_size=0.2, random_state=current_random_state)\n",
    "            train_df, val_df = train_test_split(train_val_df, test_size=1/8, random_state=current_random_state)\n",
    "            mse, rmse, mae = train_and_evaluate(best_params, train_df, val_df, test_df,\n",
    "                                                num_users, num_businesses, vocab_size, i)\n",
    "            \n",
    "            print(f\"Run {i+1} Performance on Test Set:\")\n",
    "            print(f\"  RMSE: {rmse:.4f}\")\n",
    "            print(f\"  MAE: {mae:.4f}\")\n",
    "\n",
    "            all_rmse.append(rmse)\n",
    "            all_mae.append(mae)\n",
    "\n",
    "        print(f\"\\n--- Average Performance over {num_runs} Runs ---\")\n",
    "        print(f\"Average RMSE: {np.mean(all_rmse):.4f} +/- {np.std(all_rmse):.4f}\")\n",
    "        print(f\"Average MAE: {np.mean(all_mae):.4f} +/- {np.std(all_mae):.4f}\")\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3092ab-18d1-4e0b-8e22-f2469fc8cdb1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
