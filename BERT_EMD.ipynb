{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OyMWMRvAcWMM",
        "outputId": "0864e03f-9208-4132-b6fb-0fac7d6994c7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "id": "dMf2-GClcCqd",
        "outputId": "913276d2-9f01-4394-f0c1-40fec0a2ec29"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "로드 완료: 500892개 레코드\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Encoding (CLS):   0%|          | 0/7827 [00:00<?, ?batch/s]/tmp/ipython-input-234703463.py:127: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast_ctx():\n",
            "Encoding (CLS):  55%|█████▌    | 4310/7827 [03:37<02:57, 19.85batch/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-234703463.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    137\u001b[0m                     \u001b[0;34m\"bert_embedding\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mvec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m                 }\n\u001b[0;32m--> 139\u001b[0;31m                 \u001b[0mwf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_ascii\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"✅ 완료: {OUTPUT_JSONL}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/json/__init__.py\u001b[0m in \u001b[0;36mdumps\u001b[0;34m(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0mcheck_circular\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_circular\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_nan\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mallow_nan\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0mseparators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseparators\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdefault\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msort_keys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m         **kw).encode(obj)\n\u001b[0m\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/json/encoder.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0;31m# exceptions aren't as detailed.  The list call should be roughly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0;31m# equivalent to the PySequence_Fast that ''.join() would do.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m         \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_one_shot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m             \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/json/encoder.py\u001b[0m in \u001b[0;36miterencode\u001b[0;34m(self, o, _one_shot)\u001b[0m\n\u001b[1;32m    256\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey_separator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem_separator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_keys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m                 self.skipkeys, _one_shot)\n\u001b[0;32m--> 258\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_iterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m def _make_iterencode(markers, _default, _encoder, _indent, _floatstr,\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "from typing import List, Dict\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from contextlib import nullcontext\n",
        "\n",
        "# ====== 설정 ======\n",
        "INPUT_JSON = \"/content/gdrive/MyDrive/dataset_fl.json\"\n",
        "OUTPUT_JSONL = \"/content/gdrive/MyDrive/dataset_fl_emd.jsonl\"\n",
        "\n",
        "MODEL_NAME = \"bert-base-uncased\"\n",
        "BATCH_SIZE = 64\n",
        "MAX_LENGTH = 256\n",
        "USE_FP16 = True\n",
        "NUM_WORKERS = 0\n",
        "PIN_MEMORY = False\n",
        "\n",
        "# ====== 0) 입력 파일 로더 (형식 자동 판별) ======\n",
        "def load_records_any(path: str) -> List[Dict]:\n",
        "\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        head = f.read(2048)\n",
        "    head_stripped = head.lstrip()\n",
        "\n",
        "    # 케이스 1: 리스트 JSON\n",
        "    if head_stripped.startswith('['):\n",
        "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "            return json.load(f)\n",
        "\n",
        "    # 케이스 2: JSONL 시도\n",
        "    recs = []\n",
        "    failed_jsonl = False\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for i, line in enumerate(f, 1):\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "            try:\n",
        "                recs.append(json.loads(line))\n",
        "            except json.JSONDecodeError:\n",
        "                failed_jsonl = True\n",
        "                break\n",
        "    if not failed_jsonl and recs:\n",
        "        return recs\n",
        "\n",
        "    # 케이스 3: Concatenated JSON 보정\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        raw = f.read().strip()\n",
        "    fixed = \"[\" + raw.replace(\"}\\n{\", \"},{\").replace(\"}{\", \"},{\") + \"]\"\n",
        "    try:\n",
        "        return json.loads(fixed)\n",
        "    except json.JSONDecodeError as e:\n",
        "        preview = raw[:800]\n",
        "        raise RuntimeError(\n",
        "            \"입력 파일 파싱 실패: JSON, JSONL, concat JSON 모두 불가. \"\n",
        "            f\"원인: {e}\\n파일 앞부분 미리보기:\\n{preview}\"\n",
        "        )\n",
        "\n",
        "# ====== 1) Dataset / Collate ======\n",
        "class ReviewDataset(Dataset):\n",
        "    def __init__(self, records: List[Dict]):\n",
        "        self.records = records\n",
        "    def __len__(self):\n",
        "        return len(self.records)\n",
        "    def __getitem__(self, idx):\n",
        "        r = self.records[idx]\n",
        "        return {\n",
        "            \"review_id\": r[\"review_id\"],\n",
        "            \"user_id\": r.get(\"user_id\"),\n",
        "            \"business_id\": r.get(\"business_id\"),\n",
        "            \"rating\": r.get(\"review_stars\"),  # 'stars' 필드에서 값 가져오기\n",
        "            \"review_text\": r.get(\"review_text\", \"\")\n",
        "        }\n",
        "\n",
        "def make_collate_fn(tokenizer):\n",
        "    def collate(batch: List[Dict]):\n",
        "        texts = [b.get(\"review_text\") or \"\" for b in batch]\n",
        "        enc = tokenizer(\n",
        "            texts,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=MAX_LENGTH\n",
        "        )\n",
        "        enc = {k: v.cpu() for k, v in enc.items()}\n",
        "        meta = [{k: v for k, v in b.items() if k != \"review_text\"} for b in batch]\n",
        "        return {\"meta\": meta, \"inputs\": enc}\n",
        "    return collate\n",
        "\n",
        "# ====== 2) 데이터 로드 ======\n",
        "data = load_records_any(INPUT_JSON)\n",
        "print(f\"로드 완료: {len(data)}개 레코드\")\n",
        "\n",
        "# ====== 3) 모델 준비 ======\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = AutoModel.from_pretrained(MODEL_NAME).to(device).eval()\n",
        "\n",
        "ds = ReviewDataset(data)\n",
        "dl = DataLoader(\n",
        "    ds,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=PIN_MEMORY,\n",
        "    collate_fn=make_collate_fn(tokenizer)\n",
        ")\n",
        "\n",
        "# ====== 4) 추론 & 저장(JSONL) ======\n",
        "if os.path.exists(OUTPUT_JSONL):\n",
        "    os.remove(OUTPUT_JSONL)\n",
        "\n",
        "if device.type == \"cuda\" and USE_FP16:\n",
        "    autocast_ctx = torch.cuda.amp.autocast\n",
        "else:\n",
        "    autocast_ctx = nullcontext\n",
        "\n",
        "with torch.no_grad():\n",
        "    with open(OUTPUT_JSONL, \"a\", encoding=\"utf-8\") as wf:\n",
        "        pbar = tqdm(dl, desc=\"Encoding (CLS)\", unit=\"batch\", dynamic_ncols=True)\n",
        "        for batch in pbar:\n",
        "            inputs = {k: v.to(device) for k, v in batch[\"inputs\"].items()}\n",
        "\n",
        "            with autocast_ctx():\n",
        "                out = model(**inputs)\n",
        "                cls = out.last_hidden_state[:, 0, :]\n",
        "\n",
        "            cls = cls.detach().cpu().tolist()\n",
        "            for meta, vec in zip(batch[\"meta\"], cls):\n",
        "                rec = {\n",
        "                    \"user_id\": meta.get(\"user_id\"),\n",
        "                    \"business_id\": meta.get(\"business_id\"),\n",
        "                    \"stars\": meta.get(\"rating\"),\n",
        "                    \"bert_embedding\": vec\n",
        "                }\n",
        "                wf.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "print(f\"✅ 완료: {OUTPUT_JSONL}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "N = 5 # 확인할 줄 수\n",
        "\n",
        "record_count = 0\n",
        "with open(\"/content/gdrive/MyDrive/dataset_fl.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    # 1. 파일 전체를 순회하며 개수 카운트\n",
        "    for line in f:\n",
        "        record_count += 1\n",
        "\n",
        "    # 2. 파일 포인터를 다시 맨 앞으로 이동\n",
        "    f.seek(0)\n",
        "\n",
        "    print(f\"✅ 전체 리뷰 개수: {record_count}개\")\n",
        "    print(\"-\" * 30)\n",
        "    print(f\"📄 처음 {N}개 레코드 미리보기:\")\n",
        "\n",
        "    # 3. 처음 N개 레코드 출력\n",
        "    for i, line in enumerate(f):\n",
        "        if i >= N:\n",
        "            break\n",
        "        try:\n",
        "            # JSON 형식인지 확인하며 출력 (선택 사항)\n",
        "            record = json.loads(line)\n",
        "            print(json.dumps(record, indent=2, ensure_ascii=False))\n",
        "        except json.JSONDecodeError:\n",
        "            print(f\"오류: {i+1}번째 줄은 올바른 JSON 형식이 아닙니다.\")\n",
        "            print(line.strip())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A5--vL7rB63p",
        "outputId": "97fdccf7-6360-4f4c-de93-155240d38825"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ 전체 리뷰 개수: 500892개\n",
            "------------------------------\n",
            "📄 처음 5개 레코드 미리보기:\n",
            "{\n",
            "  \"review_id\": \"OAhBYw8IQ6wlfw1owXWRWw\",\n",
            "  \"user_id\": \"1C2lxzUo1Hyye4RFIXly3g\",\n",
            "  \"business_id\": \"BVndHaLihEYbr76Z0CMEGw\",\n",
            "  \"review_stars\": 5,\n",
            "  \"review_useful\": 0,\n",
            "  \"text\": \"Great place for breakfast! I had the waffle, which was fluffy and perfect, and home fries which were nice and smashed and crunchy. Friendly waitstaff. Will definitely be back!\",\n",
            "  \"date\": 1413044526000\n",
            "}\n",
            "{\n",
            "  \"review_id\": \"mO398Ed5dpv1H5ZsKc8KXw\",\n",
            "  \"user_id\": \"yobeeTUBfaTBcnk26mXNuA\",\n",
            "  \"business_id\": \"hKameFsaXh9g8WQbv593UA\",\n",
            "  \"review_stars\": 4,\n",
            "  \"review_useful\": 0,\n",
            "  \"text\": \"Food was good- atmosphere/decor is like a fishing lodge- menu is someplace between outback & bonefish. Went on a Groupon & all 3 of our meals & calamari appetizer that we ordered were cooked perfectly- we will def be back.\",\n",
            "  \"date\": 1429111848000\n",
            "}\n",
            "{\n",
            "  \"review_id\": \"PPgbLBvi34A6m7bKJfTwhw\",\n",
            "  \"user_id\": \"3TL6HZ1JrKcNTvGDWKlrow\",\n",
            "  \"business_id\": \"GyC36Pn0Q1-qHnqXys6yFg\",\n",
            "  \"review_stars\": 1,\n",
            "  \"review_useful\": 0,\n",
            "  \"text\": \"Service and management terrible... After messing up all 4 of our orders the waiter got mad and started cursing... The manager offered our drinks to be free.. What a mess. The food was cold and nasty didn't even eat half of it and waited over an hour... I will never dine in any cracker barrel again after this experience...\",\n",
            "  \"date\": 1386422233000\n",
            "}\n",
            "{\n",
            "  \"review_id\": \"LnKr0hwejzl71QmoQyTRDQ\",\n",
            "  \"user_id\": \"7RU_xK1tEGlUvXfe0GvtEg\",\n",
            "  \"business_id\": \"hAmuto6UndVroyd_DaD-TA\",\n",
            "  \"review_stars\": 5,\n",
            "  \"review_useful\": 1,\n",
            "  \"text\": \"Not sure why it took until now for us to find this place. We started out going to the chain lobster restaurant but wanted something more authentic. A friend recommended Lobster Haven and I checked it out online. Looked like a type of place we would eat at in Massachusetts when we visit family. We were not disappointed! Service was great, fast and very friendly. Huge selection of all of the favorites. You have to visit this place if you want authentic New England seafood. You can also purchase live lobster and fresh seafood to go and cook it yourself.\",\n",
            "  \"date\": 1520122824000\n",
            "}\n",
            "{\n",
            "  \"review_id\": \"RMho6HMpdec1YgIypwWQrQ\",\n",
            "  \"user_id\": \"SNngOVGTkD34B3FAFnMv5A\",\n",
            "  \"business_id\": \"ab3pRv-b0o-BwMK2jVbH3Q\",\n",
            "  \"review_stars\": 5,\n",
            "  \"review_useful\": 0,\n",
            "  \"text\": \"My husband and I come here often. We love the food here, it's always fresh and the service is great. We highly recommend this place to anyone who loves sushi & hibachi.\",\n",
            "  \"date\": 1535071933000\n",
            "}\n"
          ]
        }
      ]
    }
  ]
}