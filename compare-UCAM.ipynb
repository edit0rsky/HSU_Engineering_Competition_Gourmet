{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4ded39a-ee29-4f94-857a-f506f032b86e",
   "metadata": {},
   "source": [
    "dataset_fl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1ee1432-d8a4-41ef-888a-6b8b0b7e5062",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alche\\anaconda3\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\alche\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Using device: cuda\n",
      "✅ JSON 파일 로드 성공.\n",
      "SBERT 모델 로딩 중...\n",
      "SBERT 모델 로딩 완료.\n",
      "SBERT 문맥 벡터 생성 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 15653/15653 [05:25<00:00, 48.11it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 1단계: 하이퍼파라미터 랜덤 서치 (10회) 시작 ---\n",
      "\n",
      "--- 시도 1/10 --- 파라미터: {'embed_dim': 32, 'hidden_dims': [128, 64], 'learning_rate': 0.002, 'batch_size': 128, 'patience': 5}\n",
      "    --> 시도 1 검증 RMSE: 0.7588\n",
      "    --> 새로운 최적 RMSE 발견: 0.7588 (파라미터: {'embed_dim': 32, 'hidden_dims': [128, 64], 'learning_rate': 0.002, 'batch_size': 128, 'patience': 5})\n",
      "\n",
      "--- 시도 2/10 --- 파라미터: {'embed_dim': 32, 'hidden_dims': [64, 32], 'learning_rate': 0.001, 'batch_size': 128, 'patience': 7}\n",
      "    --> 시도 2 검증 RMSE: 0.7539\n",
      "    --> 새로운 최적 RMSE 발견: 0.7539 (파라미터: {'embed_dim': 32, 'hidden_dims': [64, 32], 'learning_rate': 0.001, 'batch_size': 128, 'patience': 7})\n",
      "\n",
      "--- 시도 3/10 --- 파라미터: {'embed_dim': 128, 'hidden_dims': [128, 64], 'learning_rate': 0.002, 'batch_size': 256, 'patience': 5}\n",
      "    --> 시도 3 검증 RMSE: 0.8111\n",
      "\n",
      "--- 시도 4/10 --- 파라미터: {'embed_dim': 64, 'hidden_dims': [64, 32], 'learning_rate': 0.001, 'batch_size': 512, 'patience': 5}\n",
      "    --> 시도 4 검증 RMSE: 0.7754\n",
      "\n",
      "--- 시도 5/10 --- 파라미터: {'embed_dim': 64, 'hidden_dims': [128, 64], 'learning_rate': 0.002, 'batch_size': 128, 'patience': 7}\n",
      "    --> 시도 5 검증 RMSE: 0.7793\n",
      "\n",
      "--- 시도 6/10 --- 파라미터: {'embed_dim': 128, 'hidden_dims': [256, 128], 'learning_rate': 0.002, 'batch_size': 256, 'patience': 10}\n",
      "    --> 시도 6 검증 RMSE: 0.8082\n",
      "\n",
      "--- 시도 7/10 --- 파라미터: {'embed_dim': 128, 'hidden_dims': [64, 32], 'learning_rate': 0.0005, 'batch_size': 128, 'patience': 7}\n",
      "    --> 시도 7 검증 RMSE: 0.7790\n",
      "\n",
      "--- 시도 8/10 --- 파라미터: {'embed_dim': 128, 'hidden_dims': [128, 64], 'learning_rate': 0.0005, 'batch_size': 128, 'patience': 5}\n",
      "    --> 시도 8 검증 RMSE: 0.7799\n",
      "\n",
      "--- 시도 9/10 --- 파라미터: {'embed_dim': 32, 'hidden_dims': [128, 64], 'learning_rate': 0.0005, 'batch_size': 256, 'patience': 10}\n",
      "    --> 시도 9 검증 RMSE: 0.7549\n",
      "\n",
      "--- 시도 10/10 --- 파라미터: {'embed_dim': 32, 'hidden_dims': [64, 32], 'learning_rate': 0.0005, 'batch_size': 128, 'patience': 10}\n",
      "    --> 시도 10 검증 RMSE: 0.7543\n",
      "\n",
      "✅ 1단계 완료. 최적 파라미터: {'embed_dim': 32, 'hidden_dims': [64, 32], 'learning_rate': 0.001, 'batch_size': 128, 'patience': 7}\n",
      "\n",
      "--- 2단계: 최적 파라미터로 5회 반복 평가 시작 ---\n",
      "\n",
      "--- 최종 실행 1/5 시작 (Random State: 42) ---\n",
      "조기 종료 발생. (실행 final_1)\n",
      "\n",
      "✅ [UCAM] 1번째 테스트 결과: RMSE=0.7478, MAE=0.5610, MAPE=21.15%\n",
      "\n",
      "--- 최종 실행 2/5 시작 (Random State: 43) ---\n",
      "조기 종료 발생. (실행 final_2)\n",
      "\n",
      "✅ [UCAM] 2번째 테스트 결과: RMSE=0.7535, MAE=0.5641, MAPE=21.16%\n",
      "\n",
      "--- 최종 실행 3/5 시작 (Random State: 44) ---\n",
      "조기 종료 발생. (실행 final_3)\n",
      "\n",
      "✅ [UCAM] 3번째 테스트 결과: RMSE=0.7559, MAE=0.5690, MAPE=22.20%\n",
      "\n",
      "--- 최종 실행 4/5 시작 (Random State: 45) ---\n",
      "조기 종료 발생. (실행 final_4)\n",
      "\n",
      "✅ [UCAM] 4번째 테스트 결과: RMSE=0.7508, MAE=0.5614, MAPE=21.14%\n",
      "\n",
      "--- 최종 실행 5/5 시작 (Random State: 46) ---\n",
      "조기 종료 발생. (실행 final_5)\n",
      "\n",
      "✅ [UCAM] 5번째 테스트 결과: RMSE=0.7515, MAE=0.5608, MAPE=21.48%\n",
      "\n",
      "\n",
      "==================== 최종 5회 반복 평균 결과 ====================\n",
      " - 평균 RMSE : 0.7519 +/- 0.0027\n",
      " - 평균 MAE  : 0.5633 +/- 0.0031\n",
      " - 평균 MAPE : 21.43% +/- 0.41%\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import random\n",
    "import sys\n",
    "\n",
    "\n",
    "# -------------------- Device Setup --------------------\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# -------------------- 데이터 로드 및 전처리 --------------------\n",
    "try:\n",
    "    df = pd.read_json('dataset_fl.json', lines=True)\n",
    "    print(\"✅ JSON 파일 로드 성공.\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error: JSON 파일 로드 실패. (오류: {e})\")\n",
    "    sys.exit()\n",
    "\n",
    "df = df[['user_id', 'business_id', 'review_stars', 'text']].dropna()\n",
    "\n",
    "# -------------------- SBERT 모델 로딩 (한 번만) --------------------\n",
    "print(\"SBERT 모델 로딩 중...\")\n",
    "sbert_model = SentenceTransformer('all-MiniLM-L6-v2', device=device)\n",
    "print(\"SBERT 모델 로딩 완료.\")\n",
    "context_dim = sbert_model.get_sentence_embedding_dimension()\n",
    "\n",
    "# SBERT 문맥 벡터 생성 (전체 데이터에 대해 한 번만)\n",
    "print(\"SBERT 문맥 벡터 생성 중...\")\n",
    "all_context_vectors = sbert_model.encode(df['text'].tolist(), show_progress_bar=True)\n",
    "df['context_vectors'] = list(all_context_vectors)\n",
    "\n",
    "# -------------------- Dataset 클래스 정의 --------------------\n",
    "class UCAMDataset(Dataset):\n",
    "    def __init__(self, users, items, ratings, contexts):\n",
    "        self.users = users\n",
    "        self.items = items\n",
    "        self.ratings = ratings\n",
    "        self.contexts = contexts\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ratings)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            torch.tensor(self.users[idx], dtype=torch.long),\n",
    "            torch.tensor(self.items[idx], dtype=torch.long),\n",
    "            torch.tensor(self.contexts[idx], dtype=torch.float32),\n",
    "            torch.tensor(self.ratings[idx], dtype=torch.float32)\n",
    "        )\n",
    "\n",
    "# -------------------- 모델 정의 --------------------\n",
    "class UCAM(nn.Module):\n",
    "    def __init__(self, num_users, num_items, context_dim, embed_dim, hidden_dims):\n",
    "        super().__init__()\n",
    "        self.user_embed = nn.Embedding(num_users + 1, embed_dim)\n",
    "        self.item_embed = nn.Embedding(num_items + 1, embed_dim)\n",
    "        \n",
    "        layers = []\n",
    "        input_dim = embed_dim * 2 + context_dim\n",
    "        for h_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(input_dim, h_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            input_dim = h_dim\n",
    "        \n",
    "        layers.append(nn.Linear(input_dim, 1))\n",
    "        self.fc_layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, user_ids, item_ids, context_vecs):\n",
    "        u = self.user_embed(user_ids)\n",
    "        i = self.item_embed(item_ids)\n",
    "        x = torch.cat([u, i, context_vecs], dim=-1)\n",
    "        return self.fc_layers(x).squeeze()\n",
    "\n",
    "# -------------------- 평가 지표 함수 --------------------\n",
    "def evaluate_model(model, data_loader, device):\n",
    "    model.eval()\n",
    "    preds, targets = [], []\n",
    "    with torch.no_grad():\n",
    "        for users, items, contexts, ratings in data_loader:\n",
    "            users, items, contexts, ratings = users.to(device), items.to(device), contexts.to(device), ratings.to(device)\n",
    "            output = model(users, items, contexts)\n",
    "            # TypeError 해결: output을 1차원 배열로 변환\n",
    "            preds.extend(output.cpu().numpy().reshape(-1))\n",
    "            targets.extend(ratings.cpu().numpy().reshape(-1))\n",
    "\n",
    "    preds, targets = np.array(preds), np.array(targets)\n",
    "    mae = mean_absolute_error(targets, preds)\n",
    "    rmse = np.sqrt(mean_squared_error(targets, preds))\n",
    "    mape = mean_absolute_percentage_error(targets, preds)\n",
    "    return mae, rmse, mape\n",
    "\n",
    "# -------------------- 학습 및 평가 함수 (단일 실행) --------------------\n",
    "def train_and_evaluate_run(params, train_df, val_df, test_df, run_num, is_search=False):\n",
    "    # Map user/item IDs to indices for the current run's data\n",
    "    user2idx = {uid: i for i, uid in enumerate(train_df['user_id'].unique())}\n",
    "    item2idx = {iid: i for i, iid in enumerate(train_df['business_id'].unique())}\n",
    "    unknown_user_idx = len(user2idx)\n",
    "    unknown_item_idx = len(item2idx)\n",
    "    \n",
    "    train_df['user_idx'] = train_df['user_id'].map(user2idx)\n",
    "    train_df['item_idx'] = train_df['business_id'].map(item2idx)\n",
    "    val_df['user_idx'] = val_df['user_id'].map(user2idx).fillna(unknown_user_idx).astype(int)\n",
    "    val_df['item_idx'] = val_df['business_id'].map(item2idx).fillna(unknown_item_idx).astype(int)\n",
    "    test_df['user_idx'] = test_df['user_id'].map(user2idx).fillna(unknown_user_idx).astype(int)\n",
    "    test_df['item_idx'] = test_df['business_id'].map(item2idx).fillna(unknown_item_idx).astype(int)\n",
    "\n",
    "    train_dataset = UCAMDataset(train_df['user_idx'].values, train_df['item_idx'].values, train_df['review_stars'].values, np.stack(train_df['context_vectors'].values))\n",
    "    val_dataset = UCAMDataset(val_df['user_idx'].values, val_df['item_idx'].values, val_df['review_stars'].values, np.stack(val_df['context_vectors'].values))\n",
    "    test_dataset = UCAMDataset(test_df['user_idx'].values, test_df['item_idx'].values, test_df['review_stars'].values, np.stack(test_df['context_vectors'].values))\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=params['batch_size'], shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=params['batch_size'], shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=params['batch_size'], shuffle=False)\n",
    "    \n",
    "    model = UCAM(num_users=len(user2idx), num_items=len(item2idx), context_dim=context_dim,\n",
    "                 embed_dim=params['embed_dim'], hidden_dims=params['hidden_dims']).to(device)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=params['learning_rate'])\n",
    "    criterion = nn.MSELoss()\n",
    "    model_path = f'temp_ucam_model_run_{run_num}.pt'\n",
    "\n",
    "    best_val_rmse = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    patience = params['patience']\n",
    "    min_delta = 1e-4\n",
    "    epochs = 50\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for user_ids, item_ids, context_vectors, stars in train_loader:\n",
    "            user_ids, item_ids, context_vectors, stars = user_ids.to(device), item_ids.to(device), context_vectors.to(device), stars.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(user_ids, item_ids, context_vectors)\n",
    "            loss = criterion(predictions, stars)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        val_mae, val_rmse, val_mape = evaluate_model(model, val_loader, device)\n",
    "\n",
    "        if val_rmse < best_val_rmse - min_delta:\n",
    "            best_val_rmse = val_rmse\n",
    "            epochs_no_improve = 0\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve == patience:\n",
    "                if is_search:\n",
    "                    break\n",
    "                else:\n",
    "                    print(f\"조기 종료 발생. (실행 {run_num})\")\n",
    "                    break\n",
    "    \n",
    "    if os.path.exists(model_path):\n",
    "        model.load_state_dict(torch.load(model_path))\n",
    "    \n",
    "    test_mae, test_rmse, test_mape = evaluate_model(model, test_loader, device)\n",
    "    \n",
    "    if os.path.exists(model_path):\n",
    "        os.remove(model_path)\n",
    "    \n",
    "    return test_mae, test_rmse, test_mape, best_val_rmse\n",
    "\n",
    "# -------------------- 메인 실행 루틴 --------------------\n",
    "def main():\n",
    "    # -------------------- 1. 하이퍼파라미터 랜덤 서치 단계 --------------------\n",
    "    param_grid = {\n",
    "        'embed_dim': [32, 64, 128],\n",
    "        'hidden_dims': [[64, 32], [128, 64], [256, 128]],\n",
    "        'learning_rate': [0.0005, 0.001, 0.002],\n",
    "        'batch_size': [128, 256, 512],\n",
    "        'patience': [5, 7, 10]\n",
    "    }\n",
    "    num_trials = 10\n",
    "    best_params = None\n",
    "    best_val_rmse_search = float('inf')\n",
    "    \n",
    "    print(f\"\\n--- 1단계: 하이퍼파라미터 랜덤 서치 ({num_trials}회) 시작 ---\")\n",
    "    \n",
    "    # 랜덤 서치를 위한 고정된 데이터 분할\n",
    "    train_val_df_search, test_df_final = train_test_split(df, test_size=0.2, random_state=42)\n",
    "    train_df_search, val_df_search = train_test_split(train_val_df_search, test_size=1/8, random_state=42)\n",
    "    \n",
    "    for i in range(num_trials):\n",
    "        current_params = {k: random.choice(v) for k, v in param_grid.items()}\n",
    "        print(f\"\\n--- 시도 {i+1}/{num_trials} --- 파라미터: {current_params}\")\n",
    "        \n",
    "        # 최적의 파라미터는 검증 RMSE를 기준으로 찾습니다. (is_search=True)\n",
    "        _, _, _, current_val_rmse = train_and_evaluate_run(current_params, train_df_search.copy(), val_df_search.copy(), test_df_final.copy(), f'search_{i+1}', is_search=True)\n",
    "        \n",
    "        print(f\"    --> 시도 {i+1} 검증 RMSE: {current_val_rmse:.4f}\")\n",
    "        \n",
    "        if current_val_rmse < best_val_rmse_search:\n",
    "            best_val_rmse_search = current_val_rmse\n",
    "            best_params = current_params\n",
    "            print(f\"    --> 새로운 최적 RMSE 발견: {best_val_rmse_search:.4f} (파라미터: {best_params})\")\n",
    "\n",
    "    print(f\"\\n✅ 1단계 완료. 최적 파라미터: {best_params}\")\n",
    "\n",
    "    # -------------------- 2. 최적 파라미터로 5회 반복 최종 평가 단계 --------------------\n",
    "    if best_params:\n",
    "        all_rmse, all_mae, all_mape = [], [], []\n",
    "        num_runs = 5\n",
    "        print(f\"\\n--- 2단계: 최적 파라미터로 {num_runs}회 반복 평가 시작 ---\")\n",
    "\n",
    "        for i in range(num_runs):\n",
    "            # 매번 새로운 무작위 데이터 분할 사용 (random_state=42, 43, ...)\n",
    "            random_state = 42 + i\n",
    "            train_val_df, test_df = train_test_split(df, test_size=0.2, random_state=random_state)\n",
    "            train_df, val_df = train_test_split(train_val_df, test_size=1/8, random_state=random_state)\n",
    "\n",
    "            print(f\"\\n--- 최종 실행 {i+1}/{num_runs} 시작 (Random State: {random_state}) ---\")\n",
    "            \n",
    "            # 최종 테스트 성능을 측정합니다. (is_search=False)\n",
    "            test_mae, test_rmse, test_mape, _ = train_and_evaluate_run(best_params, train_df.copy(), val_df.copy(), test_df.copy(), f'final_{i+1}', is_search=False)\n",
    "            \n",
    "            print(f\"\\n✅ [UCAM] {i+1}번째 테스트 결과: RMSE={test_rmse:.4f}, MAE={test_mae:.4f}, MAPE={test_mape:.2f}%\")\n",
    "            \n",
    "            all_rmse.append(test_rmse)\n",
    "            all_mae.append(test_mae)\n",
    "            all_mape.append(test_mape)\n",
    "\n",
    "        # 최종 평균 계산 및 출력\n",
    "        avg_rmse = np.mean(all_rmse)\n",
    "        std_rmse = np.std(all_rmse)\n",
    "        avg_mae = np.mean(all_mae)\n",
    "        std_mae = np.std(all_mae)\n",
    "        avg_mape = np.mean(all_mape)\n",
    "        std_mape = np.std(all_mape)\n",
    "\n",
    "        print(\"\\n\\n==================== 최종 5회 반복 평균 결과 ====================\")\n",
    "        print(f\" - 평균 RMSE : {avg_rmse:.4f} +/- {std_rmse:.4f}\")\n",
    "        print(f\" - 평균 MAE  : {avg_mae:.4f} +/- {std_mae:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f4dc94-6466-40c0-9773-8840c22e1d64",
   "metadata": {},
   "source": [
    "dataset_la"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1012476b-877c-416f-af1a-bea65319d86d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "✅ JSON 파일 로드 성공.\n",
      "SBERT 모델 로딩 중...\n",
      "SBERT 모델 로딩 완료.\n",
      "SBERT 문맥 벡터 생성 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 8681/8681 [03:02<00:00, 47.51it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 1단계: 하이퍼파라미터 랜덤 서치 (10회) 시작 ---\n",
      "\n",
      "--- 시도 1/10 --- 파라미터: {'embed_dim': 64, 'hidden_dims': [64, 32], 'learning_rate': 0.001, 'batch_size': 128, 'patience': 7}\n",
      "    --> 시도 1 검증 RMSE: 0.7879\n",
      "    --> 새로운 최적 RMSE 발견: 0.7879 (파라미터: {'embed_dim': 64, 'hidden_dims': [64, 32], 'learning_rate': 0.001, 'batch_size': 128, 'patience': 7})\n",
      "\n",
      "--- 시도 2/10 --- 파라미터: {'embed_dim': 128, 'hidden_dims': [256, 128], 'learning_rate': 0.001, 'batch_size': 256, 'patience': 7}\n",
      "    --> 시도 2 검증 RMSE: 0.8274\n",
      "\n",
      "--- 시도 3/10 --- 파라미터: {'embed_dim': 32, 'hidden_dims': [64, 32], 'learning_rate': 0.001, 'batch_size': 256, 'patience': 5}\n",
      "    --> 시도 3 검증 RMSE: 0.7698\n",
      "    --> 새로운 최적 RMSE 발견: 0.7698 (파라미터: {'embed_dim': 32, 'hidden_dims': [64, 32], 'learning_rate': 0.001, 'batch_size': 256, 'patience': 5})\n",
      "\n",
      "--- 시도 4/10 --- 파라미터: {'embed_dim': 128, 'hidden_dims': [64, 32], 'learning_rate': 0.002, 'batch_size': 512, 'patience': 5}\n",
      "    --> 시도 4 검증 RMSE: 0.8294\n",
      "\n",
      "--- 시도 5/10 --- 파라미터: {'embed_dim': 128, 'hidden_dims': [128, 64], 'learning_rate': 0.001, 'batch_size': 256, 'patience': 10}\n",
      "    --> 시도 5 검증 RMSE: 0.8091\n",
      "\n",
      "--- 시도 6/10 --- 파라미터: {'embed_dim': 128, 'hidden_dims': [128, 64], 'learning_rate': 0.001, 'batch_size': 128, 'patience': 10}\n",
      "    --> 시도 6 검증 RMSE: 0.8175\n",
      "\n",
      "--- 시도 7/10 --- 파라미터: {'embed_dim': 64, 'hidden_dims': [128, 64], 'learning_rate': 0.0005, 'batch_size': 512, 'patience': 7}\n",
      "    --> 시도 7 검증 RMSE: 0.7960\n",
      "\n",
      "--- 시도 8/10 --- 파라미터: {'embed_dim': 128, 'hidden_dims': [128, 64], 'learning_rate': 0.0005, 'batch_size': 128, 'patience': 10}\n",
      "    --> 시도 8 검증 RMSE: 0.7998\n",
      "\n",
      "--- 시도 9/10 --- 파라미터: {'embed_dim': 64, 'hidden_dims': [64, 32], 'learning_rate': 0.001, 'batch_size': 128, 'patience': 5}\n",
      "    --> 시도 9 검증 RMSE: 0.7849\n",
      "\n",
      "--- 시도 10/10 --- 파라미터: {'embed_dim': 128, 'hidden_dims': [128, 64], 'learning_rate': 0.002, 'batch_size': 512, 'patience': 5}\n",
      "    --> 시도 10 검증 RMSE: 0.8290\n",
      "\n",
      "✅ 1단계 완료. 최적 파라미터: {'embed_dim': 32, 'hidden_dims': [64, 32], 'learning_rate': 0.001, 'batch_size': 256, 'patience': 5}\n",
      "\n",
      "--- 2단계: 최적 파라미터로 5회 반복 평가 시작 ---\n",
      "\n",
      "--- 최종 실행 1/5 시작 (Random State: 42) ---\n",
      "조기 종료 발생. (실행 final_1)\n",
      "\n",
      "✅ [UCAM] 1번째 테스트 결과: RMSE=0.7886, MAE=0.6055, MAPE=22.08%\n",
      "\n",
      "--- 최종 실행 2/5 시작 (Random State: 43) ---\n",
      "조기 종료 발생. (실행 final_2)\n",
      "\n",
      "✅ [UCAM] 2번째 테스트 결과: RMSE=0.7778, MAE=0.6006, MAPE=21.22%\n",
      "\n",
      "--- 최종 실행 3/5 시작 (Random State: 44) ---\n",
      "조기 종료 발생. (실행 final_3)\n",
      "\n",
      "✅ [UCAM] 3번째 테스트 결과: RMSE=0.7792, MAE=0.5987, MAPE=21.39%\n",
      "\n",
      "--- 최종 실행 4/5 시작 (Random State: 45) ---\n",
      "조기 종료 발생. (실행 final_4)\n",
      "\n",
      "✅ [UCAM] 4번째 테스트 결과: RMSE=0.7783, MAE=0.5994, MAPE=21.30%\n",
      "\n",
      "--- 최종 실행 5/5 시작 (Random State: 46) ---\n",
      "조기 종료 발생. (실행 final_5)\n",
      "\n",
      "✅ [UCAM] 5번째 테스트 결과: RMSE=0.7738, MAE=0.5973, MAPE=21.42%\n",
      "\n",
      "\n",
      "==================== 최종 5회 반복 평균 결과 ====================\n",
      " - 평균 RMSE : 0.7795 +/- 0.0049\n",
      " - 평균 MAE  : 0.6003 +/- 0.0028\n",
      " - 평균 MAPE : 21.48% +/- 0.31%\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import random\n",
    "import sys\n",
    "\n",
    "\n",
    "# -------------------- Device Setup --------------------\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# -------------------- 데이터 로드 및 전처리 --------------------\n",
    "try:\n",
    "    df = pd.read_json('dataset_la.json', lines=True)\n",
    "    print(\"✅ JSON 파일 로드 성공.\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error: JSON 파일 로드 실패. (오류: {e})\")\n",
    "    sys.exit()\n",
    "\n",
    "df = df[['user_id', 'business_id', 'review_stars', 'text']].dropna()\n",
    "\n",
    "# -------------------- SBERT 모델 로딩 (한 번만) --------------------\n",
    "print(\"SBERT 모델 로딩 중...\")\n",
    "sbert_model = SentenceTransformer('all-MiniLM-L6-v2', device=device)\n",
    "print(\"SBERT 모델 로딩 완료.\")\n",
    "context_dim = sbert_model.get_sentence_embedding_dimension()\n",
    "\n",
    "# SBERT 문맥 벡터 생성 (전체 데이터에 대해 한 번만)\n",
    "print(\"SBERT 문맥 벡터 생성 중...\")\n",
    "all_context_vectors = sbert_model.encode(df['text'].tolist(), show_progress_bar=True)\n",
    "df['context_vectors'] = list(all_context_vectors)\n",
    "\n",
    "# -------------------- Dataset 클래스 정의 --------------------\n",
    "class UCAMDataset(Dataset):\n",
    "    def __init__(self, users, items, ratings, contexts):\n",
    "        self.users = users\n",
    "        self.items = items\n",
    "        self.ratings = ratings\n",
    "        self.contexts = contexts\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ratings)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            torch.tensor(self.users[idx], dtype=torch.long),\n",
    "            torch.tensor(self.items[idx], dtype=torch.long),\n",
    "            torch.tensor(self.contexts[idx], dtype=torch.float32),\n",
    "            torch.tensor(self.ratings[idx], dtype=torch.float32)\n",
    "        )\n",
    "\n",
    "# -------------------- 모델 정의 --------------------\n",
    "class UCAM(nn.Module):\n",
    "    def __init__(self, num_users, num_items, context_dim, embed_dim, hidden_dims):\n",
    "        super().__init__()\n",
    "        self.user_embed = nn.Embedding(num_users + 1, embed_dim)\n",
    "        self.item_embed = nn.Embedding(num_items + 1, embed_dim)\n",
    "        \n",
    "        layers = []\n",
    "        input_dim = embed_dim * 2 + context_dim\n",
    "        for h_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(input_dim, h_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            input_dim = h_dim\n",
    "        \n",
    "        layers.append(nn.Linear(input_dim, 1))\n",
    "        self.fc_layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, user_ids, item_ids, context_vecs):\n",
    "        u = self.user_embed(user_ids)\n",
    "        i = self.item_embed(item_ids)\n",
    "        x = torch.cat([u, i, context_vecs], dim=-1)\n",
    "        return self.fc_layers(x).squeeze()\n",
    "\n",
    "# -------------------- 평가 지표 함수 --------------------\n",
    "def evaluate_model(model, data_loader, device):\n",
    "    model.eval()\n",
    "    preds, targets = [], []\n",
    "    with torch.no_grad():\n",
    "        for users, items, contexts, ratings in data_loader:\n",
    "            users, items, contexts, ratings = users.to(device), items.to(device), contexts.to(device), ratings.to(device)\n",
    "            output = model(users, items, contexts)\n",
    "            # TypeError 해결: output을 1차원 배열로 변환\n",
    "            preds.extend(output.cpu().numpy().reshape(-1))\n",
    "            targets.extend(ratings.cpu().numpy().reshape(-1))\n",
    "\n",
    "    preds, targets = np.array(preds), np.array(targets)\n",
    "    mae = mean_absolute_error(targets, preds)\n",
    "    rmse = np.sqrt(mean_squared_error(targets, preds))\n",
    "    mape = mean_absolute_percentage_error(targets, preds)\n",
    "    return mae, rmse, mape\n",
    "\n",
    "# -------------------- 학습 및 평가 함수 (단일 실행) --------------------\n",
    "def train_and_evaluate_run(params, train_df, val_df, test_df, run_num, is_search=False):\n",
    "    # Map user/item IDs to indices for the current run's data\n",
    "    user2idx = {uid: i for i, uid in enumerate(train_df['user_id'].unique())}\n",
    "    item2idx = {iid: i for i, iid in enumerate(train_df['business_id'].unique())}\n",
    "    unknown_user_idx = len(user2idx)\n",
    "    unknown_item_idx = len(item2idx)\n",
    "    \n",
    "    train_df['user_idx'] = train_df['user_id'].map(user2idx)\n",
    "    train_df['item_idx'] = train_df['business_id'].map(item2idx)\n",
    "    val_df['user_idx'] = val_df['user_id'].map(user2idx).fillna(unknown_user_idx).astype(int)\n",
    "    val_df['item_idx'] = val_df['business_id'].map(item2idx).fillna(unknown_item_idx).astype(int)\n",
    "    test_df['user_idx'] = test_df['user_id'].map(user2idx).fillna(unknown_user_idx).astype(int)\n",
    "    test_df['item_idx'] = test_df['business_id'].map(item2idx).fillna(unknown_item_idx).astype(int)\n",
    "\n",
    "    train_dataset = UCAMDataset(train_df['user_idx'].values, train_df['item_idx'].values, train_df['review_stars'].values, np.stack(train_df['context_vectors'].values))\n",
    "    val_dataset = UCAMDataset(val_df['user_idx'].values, val_df['item_idx'].values, val_df['review_stars'].values, np.stack(val_df['context_vectors'].values))\n",
    "    test_dataset = UCAMDataset(test_df['user_idx'].values, test_df['item_idx'].values, test_df['review_stars'].values, np.stack(test_df['context_vectors'].values))\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=params['batch_size'], shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=params['batch_size'], shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=params['batch_size'], shuffle=False)\n",
    "    \n",
    "    model = UCAM(num_users=len(user2idx), num_items=len(item2idx), context_dim=context_dim,\n",
    "                 embed_dim=params['embed_dim'], hidden_dims=params['hidden_dims']).to(device)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=params['learning_rate'])\n",
    "    criterion = nn.MSELoss()\n",
    "    model_path = f'temp_ucam_model_run_{run_num}.pt'\n",
    "\n",
    "    best_val_rmse = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    patience = params['patience']\n",
    "    min_delta = 1e-4\n",
    "    epochs = 50\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for user_ids, item_ids, context_vectors, stars in train_loader:\n",
    "            user_ids, item_ids, context_vectors, stars = user_ids.to(device), item_ids.to(device), context_vectors.to(device), stars.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(user_ids, item_ids, context_vectors)\n",
    "            loss = criterion(predictions, stars)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        val_mae, val_rmse, val_mape = evaluate_model(model, val_loader, device)\n",
    "\n",
    "        if val_rmse < best_val_rmse - min_delta:\n",
    "            best_val_rmse = val_rmse\n",
    "            epochs_no_improve = 0\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve == patience:\n",
    "                if is_search:\n",
    "                    break\n",
    "                else:\n",
    "                    print(f\"조기 종료 발생. (실행 {run_num})\")\n",
    "                    break\n",
    "    \n",
    "    if os.path.exists(model_path):\n",
    "        model.load_state_dict(torch.load(model_path))\n",
    "    \n",
    "    test_mae, test_rmse, test_mape = evaluate_model(model, test_loader, device)\n",
    "    \n",
    "    if os.path.exists(model_path):\n",
    "        os.remove(model_path)\n",
    "    \n",
    "    return test_mae, test_rmse, test_mape, best_val_rmse\n",
    "\n",
    "# -------------------- 메인 실행 루틴 --------------------\n",
    "def main():\n",
    "    # -------------------- 1. 하이퍼파라미터 랜덤 서치 단계 --------------------\n",
    "    param_grid = {\n",
    "        'embed_dim': [32, 64, 128],\n",
    "        'hidden_dims': [[64, 32], [128, 64], [256, 128]],\n",
    "        'learning_rate': [0.0005, 0.001, 0.002],\n",
    "        'batch_size': [128, 256, 512],\n",
    "        'patience': [5, 7, 10]\n",
    "    }\n",
    "    num_trials = 10\n",
    "    best_params = None\n",
    "    best_val_rmse_search = float('inf')\n",
    "    \n",
    "    print(f\"\\n--- 1단계: 하이퍼파라미터 랜덤 서치 ({num_trials}회) 시작 ---\")\n",
    "    \n",
    "    # 랜덤 서치를 위한 고정된 데이터 분할\n",
    "    train_val_df_search, test_df_final = train_test_split(df, test_size=0.2, random_state=42)\n",
    "    train_df_search, val_df_search = train_test_split(train_val_df_search, test_size=1/8, random_state=42)\n",
    "    \n",
    "    for i in range(num_trials):\n",
    "        current_params = {k: random.choice(v) for k, v in param_grid.items()}\n",
    "        print(f\"\\n--- 시도 {i+1}/{num_trials} --- 파라미터: {current_params}\")\n",
    "        \n",
    "        # 최적의 파라미터는 검증 RMSE를 기준으로 찾습니다. (is_search=True)\n",
    "        _, _, _, current_val_rmse = train_and_evaluate_run(current_params, train_df_search.copy(), val_df_search.copy(), test_df_final.copy(), f'search_{i+1}', is_search=True)\n",
    "        \n",
    "        print(f\"    --> 시도 {i+1} 검증 RMSE: {current_val_rmse:.4f}\")\n",
    "        \n",
    "        if current_val_rmse < best_val_rmse_search:\n",
    "            best_val_rmse_search = current_val_rmse\n",
    "            best_params = current_params\n",
    "            print(f\"    --> 새로운 최적 RMSE 발견: {best_val_rmse_search:.4f} (파라미터: {best_params})\")\n",
    "\n",
    "    print(f\"\\n✅ 1단계 완료. 최적 파라미터: {best_params}\")\n",
    "\n",
    "    # -------------------- 2. 최적 파라미터로 5회 반복 최종 평가 단계 --------------------\n",
    "    if best_params:\n",
    "        all_rmse, all_mae, all_mape = [], [], []\n",
    "        num_runs = 5\n",
    "        print(f\"\\n--- 2단계: 최적 파라미터로 {num_runs}회 반복 평가 시작 ---\")\n",
    "\n",
    "        for i in range(num_runs):\n",
    "            # 매번 새로운 무작위 데이터 분할 사용 (random_state=42, 43, ...)\n",
    "            random_state = 42 + i\n",
    "            train_val_df, test_df = train_test_split(df, test_size=0.2, random_state=random_state)\n",
    "            train_df, val_df = train_test_split(train_val_df, test_size=1/8, random_state=random_state)\n",
    "\n",
    "            print(f\"\\n--- 최종 실행 {i+1}/{num_runs} 시작 (Random State: {random_state}) ---\")\n",
    "            \n",
    "            # 최종 테스트 성능을 측정합니다. (is_search=False)\n",
    "            test_mae, test_rmse, test_mape, _ = train_and_evaluate_run(best_params, train_df.copy(), val_df.copy(), test_df.copy(), f'final_{i+1}', is_search=False)\n",
    "            \n",
    "            print(f\"\\n✅ [UCAM] {i+1}번째 테스트 결과: RMSE={test_rmse:.4f}, MAE={test_mae:.4f}, MAPE={test_mape:.2f}%\")\n",
    "            \n",
    "            all_rmse.append(test_rmse)\n",
    "            all_mae.append(test_mae)\n",
    "            all_mape.append(test_mape)\n",
    "\n",
    "        # 최종 평균 계산 및 출력\n",
    "        avg_rmse = np.mean(all_rmse)\n",
    "        std_rmse = np.std(all_rmse)\n",
    "        avg_mae = np.mean(all_mae)\n",
    "        std_mae = np.std(all_mae)\n",
    "        avg_mape = np.mean(all_mape)\n",
    "        std_mape = np.std(all_mape)\n",
    "\n",
    "        print(\"\\n\\n==================== 최종 5회 반복 평균 결과 ====================\")\n",
    "        print(f\" - 평균 RMSE : {avg_rmse:.4f} +/- {std_rmse:.4f}\")\n",
    "        print(f\" - 평균 MAE  : {avg_mae:.4f} +/- {std_mae:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0059d4-1656-4acf-8cc3-a090c0af5153",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
