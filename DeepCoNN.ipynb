{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HjpTrMZoilJJ"
      },
      "outputs": [],
      "source": [
        "!pip install gensim torch pandas numpy scikit-learn tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1pb_YpE4QOgb",
        "outputId": "da9d65f0-507b-473a-b724-93c334f22232"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Device: cuda\n",
            "[*] 데이터 로드…\n",
            "                  user_id             business_id  rating  \\\n",
            "0  smOvOajNG0lS4Pq7d8g4JQ  RZtGWDLCAtuipwaZ-UfjmQ       4   \n",
            "1  IQsF3Rc6IgCzjVV9DE8KXg  eFvzHawVJofxSnD7TgbZtg       5   \n",
            "\n",
            "                                                text  \n",
            "0  Good food--loved the gnocchi with marinara\\nth...  \n",
            "1  My absolute favorite cafe in the city. Their b...  \n",
            "총 리뷰 수: 447,796\n",
            "[*] 전처리/임베딩…\n",
            "[*] 사용자/아이템 문서 생성…\n",
            "[*] 토큰화/단어 카운트…\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "tokenize: 100%|██████████| 27807/27807 [00:19<00:00, 1424.16it/s]\n",
            "tokenize: 100%|██████████| 6831/6831 [00:19<00:00, 341.60it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[+] Vocab size: 80,562\n",
            "[*] Word2Vec 로딩…\n",
            "[+] Embedding matrix: (80562, 300) | hit=63,204 / oov=17,356\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "make user seqs: 100%|██████████| 27807/27807 [00:05<00:00, 5371.04it/s]\n",
            "make item seqs: 100%|██████████| 6831/6831 [00:04<00:00, 1588.40it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[*] 학습/검증/조기종료/테스트…\n",
            "[INFO] Start training...\n",
            "Epoch 01 | Train MSE 69.0589 | Val MSE 1.1418 | RMSE 1.0686 | MAE 0.8622 | MAPE 33.62%\n",
            "  --> Improved. Save model (Val RMSE: 1.0686)\n",
            "Epoch 02 | Train MSE 1.1953 | Val MSE 1.1106 | RMSE 1.0538 | MAE 0.8513 | MAPE 33.11%\n",
            "  --> Improved. Save model (Val RMSE: 1.0538)\n",
            "Epoch 03 | Train MSE 1.1397 | Val MSE 1.1736 | RMSE 1.0833 | MAE 0.8204 | MAPE 35.52%\n",
            "  --> No improvement (1/5)\n",
            "Epoch 04 | Train MSE 1.1132 | Val MSE 1.0913 | RMSE 1.0447 | MAE 0.8065 | MAPE 33.65%\n",
            "  --> Improved. Save model (Val RMSE: 1.0447)\n",
            "Epoch 05 | Train MSE 1.0956 | Val MSE 1.0937 | RMSE 1.0458 | MAE 0.8097 | MAPE 33.37%\n",
            "  --> No improvement (1/5)\n",
            "Epoch 06 | Train MSE 1.0798 | Val MSE 1.0849 | RMSE 1.0416 | MAE 0.8157 | MAPE 33.18%\n",
            "  --> Improved. Save model (Val RMSE: 1.0416)\n",
            "Epoch 07 | Train MSE 1.0693 | Val MSE 1.2218 | RMSE 1.1054 | MAE 0.8252 | MAPE 35.75%\n",
            "  --> No improvement (1/5)\n",
            "Epoch 08 | Train MSE 1.0586 | Val MSE 1.1905 | RMSE 1.0911 | MAE 0.8180 | MAPE 35.02%\n",
            "  --> No improvement (2/5)\n",
            "Epoch 09 | Train MSE 1.0534 | Val MSE 1.1367 | RMSE 1.0661 | MAE 0.8642 | MAPE 31.77%\n",
            "  --> No improvement (3/5)\n",
            "Epoch 10 | Train MSE 1.0442 | Val MSE 1.1034 | RMSE 1.0504 | MAE 0.8016 | MAPE 33.39%\n",
            "  --> No improvement (4/5)\n",
            "Epoch 11 | Train MSE 1.0370 | Val MSE 1.0914 | RMSE 1.0447 | MAE 0.8356 | MAPE 31.91%\n",
            "  --> No improvement (5/5)\n",
            "[INFO] Early stopping.\n",
            "[INFO] Loaded best model: /content/drive/MyDrive/best_deepconn_textcnn_fm.pt\n",
            "\n",
            "✅ [DeepCoNN: TextCNN+FM] 최종 테스트 지표 (클램프 1~5)\n",
            "   - MSE  : 1.0798\n",
            "   - RMSE : 1.0391\n",
            "   - MAE  : 0.8143\n",
            "   - MAPE : 32.97%\n"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "import os, re, json, math, random\n",
        "from collections import Counter\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "\n",
        "# ==================== 0) 재현성 & 디바이스 ====================\n",
        "SEED = 42\n",
        "random.seed(SEED); np.random.seed(SEED)\n",
        "torch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"[INFO] Device: {device}\")\n",
        "\n",
        "# ==================== 1) 경로/하이퍼파라미터 ====================\n",
        "DATA_PATH = \"/content/drive/MyDrive/review_business_5up_with_text.json\"       # 파일경로수정\n",
        "GOOGLE_NEWS_BIN = \"/content/drive/MyDrive/GoogleNews-vectors-negative300.bin.gz\" # https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit 다운\n",
        "WORK_DIR = Path(\"/content/drive/MyDrive/\") \n",
        "WORK_DIR.mkdir(parents=True, exist_ok=True)\n",
        "BEST_MODEL_PATH = str(WORK_DIR / \"best_deepconn.pt\")\n",
        "\n",
        "# 전처리\n",
        "MIN_COUNT = 5\n",
        "MAX_USER_LEN = 1000\n",
        "MAX_ITEM_LEN = 1000\n",
        "EMB_DIM = 300  # GoogleNews\n",
        "\n",
        "# 학습\n",
        "BATCH_SIZE = 256\n",
        "EPOCHS = 50\n",
        "LR = 2e-3\n",
        "DROPOUT = 0.5\n",
        "OUT_DIM = 50\n",
        "CONV_KERNEL = 3\n",
        "CONV_FILTERS = 100\n",
        "FREEZE_EMB = False\n",
        "\n",
        "# 조기 종료\n",
        "PATIENCE = 5\n",
        "MIN_DELTA = 1e-3\n",
        "\n",
        "# ==================== 2) 유틸 함수 ====================\n",
        "def simple_tokenize(text: str) -> List[str]:\n",
        "    return re.findall(r\"[A-Za-z0-9']+\", str(text))\n",
        "\n",
        "def load_reviews(path: str) -> pd.DataFrame:\n",
        "    rows = []\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        head = f.read(2048); f.seek(0)\n",
        "        if head.strip().startswith(\"[\"):\n",
        "            rows = json.load(f)  # JSON array\n",
        "        else:\n",
        "            for line in f:       # JSONL\n",
        "                line = line.strip()\n",
        "                if not line: continue\n",
        "                try:\n",
        "                    rows.append(json.loads(line))\n",
        "                except json.JSONDecodeError:\n",
        "                    pass\n",
        "    df = pd.DataFrame(rows)\n",
        "    # 표준 컬럼 이름 맞추기\n",
        "    if \"stars\" in df.columns and \"rating\" not in df.columns:\n",
        "        df = df.rename(columns={\"stars\": \"rating\"})\n",
        "    required = [\"user_id\", \"business_id\", \"rating\", \"text\"]\n",
        "    miss = [c for c in required if c not in df.columns]\n",
        "    if miss:\n",
        "        raise ValueError(f\"필수 컬럼 누락: {miss} | 현재: {list(df.columns)[:12]}\")\n",
        "    return df[required].dropna().reset_index(drop=True)\n",
        "\n",
        "def mean_absolute_percentage_error(y_true, y_pred, eps=1e-10):\n",
        "    y_true = np.asarray(y_true, dtype=np.float32)\n",
        "    y_pred = np.asarray(y_pred, dtype=np.float32)\n",
        "    denom = np.clip(np.abs(y_true), eps, None)  # 0-division 방지\n",
        "    return float(np.mean(np.abs((y_true - y_pred) / denom)) * 100.0)\n",
        "\n",
        "# ==================== 3) Stage-1: 전처리/임베딩 ====================\n",
        "def build_data(df: pd.DataFrame):\n",
        "    # 사용자/아이템 문서 구성\n",
        "    print(\"[*] 사용자/아이템 문서 생성…\")\n",
        "    user_docs = df.groupby(\"user_id\")[\"text\"].apply(lambda s: \" \".join(map(str, s))).to_dict()\n",
        "    item_docs = df.groupby(\"business_id\")[\"text\"].apply(lambda s: \" \".join(map(str, s))).to_dict()\n",
        "\n",
        "    # 토큰화 & vocab\n",
        "    print(\"[*] 토큰화/단어 카운트…\")\n",
        "    word_counter = Counter()\n",
        "    def tokenize_and_count(docs: Dict[str,str]):\n",
        "        tokenized = {}\n",
        "        for k, txt in tqdm(docs.items(), desc=\"tokenize\"):\n",
        "            toks = simple_tokenize(txt)\n",
        "            tokenized[k] = toks\n",
        "            word_counter.update(toks)\n",
        "        return tokenized\n",
        "\n",
        "    user_tokens = tokenize_and_count(user_docs)\n",
        "    item_tokens = tokenize_and_count(item_docs)\n",
        "\n",
        "    PAD, UNK = \"<PAD>\", \"<UNK>\"\n",
        "    vocab = {PAD: 0, UNK: 1}\n",
        "    for w, c in word_counter.items():\n",
        "        if c >= MIN_COUNT and w not in vocab:\n",
        "            vocab[w] = len(vocab)\n",
        "    print(f\"[+] Vocab size: {len(vocab):,}\")\n",
        "\n",
        "    # Word2Vec 로드 & 임베딩 매트릭스\n",
        "    if not os.path.exists(GOOGLE_NEWS_BIN):\n",
        "        raise FileNotFoundError(f\"사전학습 벡터 필요: {GOOGLE_NEWS_BIN}\")\n",
        "    print(\"[*] Word2Vec 로딩…\")\n",
        "    kv = KeyedVectors.load_word2vec_format(GOOGLE_NEWS_BIN, binary=True)\n",
        "    assert kv.vector_size == EMB_DIM\n",
        "\n",
        "    emb = np.random.normal(0, 0.01, size=(len(vocab), EMB_DIM)).astype(np.float32)\n",
        "    emb[vocab[PAD]] = 0.0\n",
        "    hit = 0\n",
        "    for w, idx in vocab.items():\n",
        "        if w in (PAD, UNK): continue\n",
        "        if w in kv: emb[idx] = kv[w]; hit += 1\n",
        "    print(f\"[+] Embedding matrix: {emb.shape} | hit={hit:,} / oov={len(vocab)-hit-2:,}\")\n",
        "\n",
        "    # 시퀀스 변환\n",
        "    def to_ids(tokens: List[str], vocab: Dict[str,int], max_len: int):\n",
        "        ids = [vocab.get(w, 1) for w in tokens]  # UNK=1\n",
        "        return ids[:max_len] if len(ids) > max_len else ids\n",
        "\n",
        "    user_seqs = {uid: to_ids(toks, vocab, MAX_USER_LEN) for uid, toks in tqdm(user_tokens.items(), desc=\"make user seqs\")}\n",
        "    item_seqs = {bid: to_ids(toks, vocab, MAX_ITEM_LEN) for bid, toks in tqdm(item_tokens.items(), desc=\"make item seqs\")}\n",
        "    return vocab, emb, user_seqs, item_seqs\n",
        "\n",
        "# ==================== 4) Dataset / Model 정의 ====================\n",
        "class DeepConnDataset(Dataset):\n",
        "    def __init__(self, frame: pd.DataFrame, user_seqs, item_seqs, pad_idx=0):\n",
        "        self.df = frame.reset_index(drop=True)\n",
        "        self.user_seqs = user_seqs\n",
        "        self.item_seqs = item_seqs\n",
        "        self.pad = pad_idx\n",
        "\n",
        "    def __len__(self): return len(self.df)\n",
        "\n",
        "    def _pad(self, seq, max_len):\n",
        "        s = seq[:max_len]\n",
        "        if len(s) < max_len:\n",
        "            s = s + [self.pad] * (max_len - len(s))\n",
        "        return torch.tensor(s, dtype=torch.long)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        r = self.df.iloc[i]\n",
        "        u_seq = self.user_seqs.get(r[\"user_id\"], [])\n",
        "        v_seq = self.item_seqs.get(r[\"business_id\"], [])\n",
        "        x_u = self._pad(u_seq, MAX_USER_LEN)\n",
        "        x_v = self._pad(v_seq, MAX_ITEM_LEN)\n",
        "        y = torch.tensor(float(r[\"rating\"]), dtype=torch.float32)\n",
        "        return x_u, x_v, y\n",
        "\n",
        "class TextCNN(nn.Module):\n",
        "    def __init__(self, emb_weights: np.ndarray, out_dim=50, conv_filters=100, kernel_size=3, dropout=0.5, freeze=False):\n",
        "        super().__init__()\n",
        "        vocab_size, emb_dim = emb_weights.shape\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
        "        self.embedding.weight.data.copy_(torch.tensor(emb_weights))\n",
        "        self.embedding.weight.requires_grad = not freeze\n",
        "\n",
        "        self.conv = nn.Conv1d(emb_dim, conv_filters, kernel_size=kernel_size, stride=1, padding=0)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc = nn.Linear(conv_filters, out_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        emb = self.embedding(x)        # (B, L, E)\n",
        "        emb = emb.transpose(1, 2)      # (B, E, L)\n",
        "        h = self.conv(emb)             # (B, F, L')\n",
        "        h = self.relu(h)\n",
        "        h = torch.max(h, dim=2).values # Global Max Pool -> (B, F)\n",
        "        h = self.dropout(h)\n",
        "        out = self.fc(h)               # (B, out_dim)\n",
        "        return out\n",
        "\n",
        "class DeepCoNN_TextCNN_FM(nn.Module):\n",
        "    def __init__(self, emb_weights, out_dim=50, conv_filters=100, kernel_size=3, dropout=0.5, fm_k=40, freeze=False):\n",
        "        super().__init__()\n",
        "        self.user_net = TextCNN(emb_weights, out_dim, conv_filters, kernel_size, dropout, freeze)\n",
        "        self.item_net = TextCNN(emb_weights, out_dim, conv_filters, kernel_size, dropout, freeze)\n",
        "        F = out_dim * 2\n",
        "        self.fm_bias = nn.Parameter(torch.zeros(1))\n",
        "        self.fm_w = nn.Linear(F, 1, bias=False)\n",
        "        self.fm_V = nn.Parameter(torch.randn(F, fm_k) * 0.01)\n",
        "\n",
        "    def fm_forward(self, z):\n",
        "        linear = self.fm_w(z) + self.fm_bias\n",
        "        vz = torch.matmul(z, self.fm_V)                         # (B, k)\n",
        "        pairwise = 0.5 * torch.sum(vz * vz - torch.matmul(z*z, self.fm_V*self.fm_V), dim=1, keepdim=True)\n",
        "        return linear + pairwise\n",
        "\n",
        "    def forward(self, u_ids, v_ids):\n",
        "        xu = self.user_net(u_ids)\n",
        "        yi = self.item_net(v_ids)\n",
        "        z  = torch.cat([xu, yi], dim=1)                         # (B, 2*out_dim)\n",
        "        y  = self.fm_forward(z).squeeze(1)\n",
        "        return y\n",
        "\n",
        "# ==================== 5) 학습/검증/테스트 루틴 ====================\n",
        "def eval_on_loader(model, loader, device, clamp_to_star=False):\n",
        "    model.eval()\n",
        "    yt, yp = [], []\n",
        "    with torch.no_grad():\n",
        "        for u, v, y in loader:\n",
        "            u, v = u.to(device), v.to(device)\n",
        "            p = model(u, v)\n",
        "            if clamp_to_star:\n",
        "                p = torch.clamp(p, 1.0, 5.0)  # 별점 범위로 제한\n",
        "            yp.append(p.cpu().numpy())\n",
        "            yt.append(y.numpy())\n",
        "    yt = np.concatenate(yt); yp = np.concatenate(yp)\n",
        "    mse  = float(mean_squared_error(yt, yp))\n",
        "    rmse = float(math.sqrt(mse))\n",
        "    mae  = float(mean_absolute_error(yt, yp))\n",
        "    mape = mean_absolute_percentage_error(yt, yp)\n",
        "    return mse, rmse, mae, mape\n",
        "\n",
        "def train_with_early_stop(df, emb_matrix):\n",
        "    # Split 70/10/20\n",
        "    train_df, test_df = train_test_split(df, test_size=0.1, random_state=SEED)\n",
        "    train_df, val_df  = train_test_split(train_df, test_size=0.2222, random_state=SEED)\n",
        "\n",
        "    train_ds = DeepConnDataset(train_df, user_seqs, item_seqs, pad_idx=0)\n",
        "    val_ds   = DeepConnDataset(val_df,   user_seqs, item_seqs, pad_idx=0)\n",
        "    test_ds  = DeepConnDataset(test_df,  user_seqs, item_seqs, pad_idx=0)\n",
        "\n",
        "    num_workers = 2 if torch.cuda.is_available() else 0\n",
        "    pin_memory  = True if torch.cuda.is_available() else False\n",
        "\n",
        "    train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
        "                          num_workers=num_workers, pin_memory=pin_memory, persistent_workers=bool(num_workers))\n",
        "    val_dl   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False,\n",
        "                          num_workers=num_workers, pin_memory=pin_memory, persistent_workers=bool(num_workers))\n",
        "    test_dl  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False,\n",
        "                          num_workers=num_workers, pin_memory=pin_memory, persistent_workers=bool(num_workers))\n",
        "\n",
        "    model = DeepCoNN_TextCNN_FM(\n",
        "        emb_matrix, out_dim=OUT_DIM, conv_filters=CONV_FILTERS,\n",
        "        kernel_size=CONV_KERNEL, dropout=DROPOUT, fm_k=40, freeze=FREEZE_EMB\n",
        "    ).to(device)\n",
        "\n",
        "    optimizer = torch.optim.RMSprop(model.parameters(), lr=LR)\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    best_val_rmse = float(\"inf\")\n",
        "    epochs_no_improve = 0\n",
        "\n",
        "    print(\"[INFO] Start training...\")\n",
        "    for epoch in range(1, EPOCHS + 1):\n",
        "        model.train()\n",
        "        total_loss, n = 0.0, 0\n",
        "        for u, v, y in train_dl:\n",
        "            u, v, y = u.to(device), v.to(device), y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            pred = model(u, v)\n",
        "            loss = criterion(pred, y)\n",
        "            loss.backward()\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item() * len(y)\n",
        "            n += len(y)\n",
        "\n",
        "        # ----- Validation -----\n",
        "        val_mse, val_rmse, val_mae, val_mape = eval_on_loader(model, val_dl, device, clamp_to_star=False)\n",
        "        train_mse = total_loss / max(n, 1)\n",
        "\n",
        "        print(f\"Epoch {epoch:02d} | Train MSE {train_mse:.4f} | \"\n",
        "              f\"Val MSE {val_mse:.4f} | RMSE {val_rmse:.4f} | MAE {val_mae:.4f} | MAPE {val_mape:.2f}%\")\n",
        "\n",
        "        # Early Stopping\n",
        "        if val_rmse < best_val_rmse - MIN_DELTA:\n",
        "            best_val_rmse = val_rmse\n",
        "            epochs_no_improve = 0\n",
        "            torch.save(model.state_dict(), BEST_MODEL_PATH)\n",
        "            print(f\"  --> Improved. Save model (Val RMSE: {best_val_rmse:.4f})\")\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "            print(f\"  --> No improvement ({epochs_no_improve}/{PATIENCE})\")\n",
        "            if epochs_no_improve >= PATIENCE:\n",
        "                print(\"[INFO] Early stopping.\")\n",
        "                break\n",
        "\n",
        "    # ----- Test -----\n",
        "    if os.path.exists(BEST_MODEL_PATH):\n",
        "        model.load_state_dict(torch.load(BEST_MODEL_PATH, map_location=device))\n",
        "        print(f\"[INFO] Loaded best model: {BEST_MODEL_PATH}\")\n",
        "    test_mse, test_rmse, test_mae, test_mape = eval_on_loader(model, test_dl, device, clamp_to_star=True)\n",
        "\n",
        "    print(\"\\n✅ [DeepCoNN: TextCNN+FM] 최종 테스트 지표 (클램프 1~5)\")\n",
        "    print(f\"   - MSE  : {test_mse:.4f}\")\n",
        "    print(f\"   - RMSE : {test_rmse:.4f}\")\n",
        "    print(f\"   - MAE  : {test_mae:.4f}\")\n",
        "    print(f\"   - MAPE : {test_mape:.2f}%\")\n",
        "\n",
        "# ==================== 6) 메인 플로우 ====================\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"[*] 데이터 로드…\")\n",
        "    df = load_reviews(DATA_PATH)\n",
        "    print(df.head(2))\n",
        "    print(f\"총 리뷰 수: {len(df):,}\")\n",
        "\n",
        "    print(\"[*] 전처리/임베딩…\")\n",
        "    vocab, emb_matrix, user_seqs, item_seqs = build_data(df)\n",
        "\n",
        "    print(\"[*] 학습/검증/조기종료/테스트…\")\n",
        "    train_with_early_stop(df, emb_matrix)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
